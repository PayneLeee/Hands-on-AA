{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3081b427",
      "metadata": {},
      "source": [
        "# ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹åŸºç¡€åŸç†\n",
        "\n",
        "## 1. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)\n",
        "\n",
        "**æ ¸å¿ƒæ€æƒ³**ï¼šè®©æ¨¡å‹å…³æ³¨åˆ°å¯¹å½“å‰ä»»åŠ¡æœ€æœ‰å¸®åŠ©çš„å†…å®¹\n",
        "\n",
        "æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨ä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡å…³ç³»ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Transformer æ¶æ„\n",
        "\n",
        "**æ ¸å¿ƒç‰¹ç‚¹**ï¼šä»…ä»…ä¾èµ–æ³¨æ„åŠ›æœºåˆ¶æ¥å»ºæ¨¡åºåˆ—ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»\n",
        "\n",
        "### 2.1 å…³é”®ç»„ä»¶\n",
        "\n",
        "- **Q (Query) å‘é‡**ï¼šæŸ¥è¯¢å‘é‡ï¼Œè¡¨ç¤º\"æˆ‘è¦æ‰¾ä»€ä¹ˆ\"\n",
        "- **K (Key) å‘é‡**ï¼šé”®å‘é‡ï¼Œè¡¨ç¤º\"æˆ‘æ˜¯ä»€ä¹ˆ\"\n",
        "- **V (Value) å‘é‡**ï¼šå€¼å‘é‡ï¼Œè¡¨ç¤º\"æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆ\"\n",
        "\n",
        "### 2.2 å·¥ä½œåŸç†\n",
        "\n",
        "é€šè¿‡è®¡ç®— Qã€Kã€V ä¹‹é—´çš„æƒé‡å…³ç³»ï¼Œæ¨¡å‹èƒ½å¤Ÿï¼š\n",
        "- æ•æ‰é•¿è·ç¦»ä¾èµ–\n",
        "- å¹¶è¡Œå¤„ç†åºåˆ—\n",
        "- æé«˜è®­ç»ƒæ•ˆç‡\n",
        "\n",
        "---\n",
        "\n",
        "## 3. æ–‡æœ¬ç”ŸæˆåŸç†\n",
        "\n",
        "### 3.1 Tokenï¼ˆè¯å…ƒï¼‰\n",
        "\n",
        "**å®šä¹‰**ï¼šè¯æ±‡è¡¨ä¸­æœ€å°çš„å•å…ƒ\n",
        "\n",
        "- **Tokenizeï¼ˆåˆ†è¯ï¼‰**ï¼šå°†å®Œæ•´å•è¯åˆ†è§£åˆ°æ›´å°çš„éƒ¨åˆ†\n",
        "  - å¸¸ç”¨å·¥å…·ï¼šspaCyã€BERT tokenizer ç­‰\n",
        "  - ç›®çš„ï¼šæé«˜æ¨¡å‹å¯¹è¯æ±‡çš„ç†è§£å’Œå¤„ç†èƒ½åŠ›\n",
        "\n",
        "### 3.2 è‡ªå›å½’ç”Ÿæˆ (Autoregressive Generation)\n",
        "\n",
        "**å·¥ä½œåŸç†**ï¼š\n",
        "1. LLM ä¸€æ¬¡åªé¢„æµ‹ä¸€ä¸ª token\n",
        "2. æ ¹æ®å·²æœ‰çš„ token åºåˆ—æ¥è®¡ç®—ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ\n",
        "3. ä¸€èˆ¬æƒ…å†µä¸‹é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯\n",
        "4. è¿‡ç¨‹æ˜¯è¿­ä»£é‡å¤çš„ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶\n",
        "\n",
        "**åœæ­¢æ¡ä»¶**ï¼š\n",
        "- ç†æƒ³æƒ…å†µä¸‹ç”±æ¨¡å‹å†³å®š\n",
        "- è¾“å‡ºä¸€ä¸ªç»“æŸåºåˆ—æ ‡è®°ï¼ˆEOS - End of Sequenceï¼‰\n",
        "- æˆ–è¾¾åˆ°æœ€å¤§ç”Ÿæˆé•¿åº¦é™åˆ¶\n",
        "\n",
        "### 3.3 ç”Ÿæˆè¿‡ç¨‹çš„å¹²é¢„\n",
        "\n",
        "#### 3.3.1 Temperatureï¼ˆæ¸©åº¦å‚æ•°ï¼‰\n",
        "\n",
        "**ä½œç”¨**ï¼šé€šè¿‡æ¸©åº¦å€¼æ¥æ§åˆ¶ç”Ÿæˆç»“æœçš„éšæœºæ€§\n",
        "\n",
        "- **æ¸©åº¦è¶Šé«˜**ï¼šæ¦‚ç‡åˆ†å¸ƒè¶Šå¹³æ»‘ï¼Œéšæœºæ€§å¢åŠ ï¼Œè¾“å‡ºæ›´å¤šæ ·åŒ–\n",
        "- **æ¸©åº¦è¶Šä½**ï¼šæ¦‚ç‡åˆ†å¸ƒè¶Šå°–é”ï¼Œéšæœºæ€§é™ä½ï¼Œè¾“å‡ºæ›´ç¡®å®š\n",
        "\n",
        "**å…¸å‹å–å€¼**ï¼š\n",
        "- `temperature = 0.1-0.3`ï¼šæ›´ç¡®å®šã€æ›´ä¿å®ˆçš„è¾“å‡º\n",
        "- `temperature = 0.7-0.9`ï¼šå¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§\n",
        "- `temperature > 1.0`ï¼šæ›´éšæœºã€æ›´åˆ›é€ æ€§çš„è¾“å‡º\n",
        "\n",
        "#### 3.3.2 é‡‡æ ·ç­–ç•¥\n",
        "\n",
        "**1. è´ªå¿ƒç­–ç•¥ (Greedy Strategy)**\n",
        "- å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å•è¯\n",
        "- **ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€ç¡®å®šæ€§å¼º\n",
        "- **ç¼ºç‚¹**ï¼šå¯èƒ½é™·å…¥é‡å¤å¾ªç¯ï¼Œç¼ºä¹å¤šæ ·æ€§\n",
        "\n",
        "**2. æ³¢æŸæœç´¢ (Beam Search)**\n",
        "- æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™æœ€å¯èƒ½çš„ `num_beams` ä¸ªè¯\n",
        "- ä»ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åºåˆ—\n",
        "- **ä¼˜ç‚¹**ï¼šåœ¨ç¡®å®šæ€§å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡\n",
        "- **ç¼ºç‚¹**ï¼šè®¡ç®—æˆæœ¬è¾ƒé«˜\n",
        "\n",
        "**3. Top-K é‡‡æ ·**\n",
        "- ä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ªè¯ä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè¯\n",
        "- **ä¼˜ç‚¹**ï¼šç®€å•æœ‰æ•ˆï¼Œæ§åˆ¶å€™é€‰è¯æ•°é‡\n",
        "- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦ä¸€å®šéšæœºæ€§çš„ç”Ÿæˆä»»åŠ¡\n",
        "\n",
        "**4. Top-Pï¼ˆæ ¸é‡‡æ ·ï¼ŒNucleus Samplingï¼‰**\n",
        "- ä»æ¦‚ç‡ç´¯ç§¯è¾¾åˆ° P çš„æœ€å°è¯é›†ä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè¯\n",
        "- åŠ¨æ€è°ƒæ•´é‡‡æ ·èŒƒå›´\n",
        "- **ä¼˜ç‚¹**ï¼šè‡ªé€‚åº”é€‰æ‹©å€™é€‰è¯æ•°é‡ï¼Œæ›´çµæ´»\n",
        "- **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§çš„ä»»åŠ¡\n",
        "\n",
        "#### 3.3.3 ç”Ÿæˆç­–ç•¥å»ºè®®\n",
        "\n",
        "| åœºæ™¯ | æ¨èç­–ç•¥ | å‚æ•°è®¾ç½® |\n",
        "|------|---------|---------|\n",
        "| **åˆ›é€ æ€§ä»»åŠ¡**<br>ï¼ˆå†™è¯—ã€åˆ›æ„å†™ä½œï¼‰ | Top-P æˆ– Top-K | `temperature=0.9-1.2`<br>`top_p=0.9-0.95` |\n",
        "| **å‡†ç¡®æ€§ä»»åŠ¡**<br>ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰ | è´ªå¿ƒæˆ–æ³¢æŸæœç´¢ | `temperature=0.1-0.3`<br>`num_beams=3-5` |\n",
        "| **å¹³è¡¡ä»»åŠ¡**<br>ï¼ˆå¯¹è¯ã€é—®ç­”ï¼‰ | Top-P | `temperature=0.7-0.9`<br>`top_p=0.9` |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. æ ¸å¿ƒè¦ç‚¹æ€»ç»“\n",
        "\n",
        "### 4.1 æ³¨æ„åŠ›æœºåˆ¶\n",
        "- âœ… è®©æ¨¡å‹å…³æ³¨é‡è¦ä¿¡æ¯\n",
        "- âœ… æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»\n",
        "- âœ… æé«˜æ¨¡å‹ç†è§£èƒ½åŠ›\n",
        "\n",
        "### 4.2 Transformer æ¶æ„\n",
        "- âœ… åŸºäºæ³¨æ„åŠ›æœºåˆ¶\n",
        "- âœ… Qã€Kã€V å‘é‡æ˜¯å…³é”®\n",
        "- âœ… æ”¯æŒå¹¶è¡Œå¤„ç†\n",
        "\n",
        "### 4.3 æ–‡æœ¬ç”Ÿæˆ\n",
        "- âœ… Token æ˜¯æœ€å°å•å…ƒ\n",
        "- âœ… è‡ªå›å½’é€è¯ç”Ÿæˆ\n",
        "- âœ… å¯é€šè¿‡å‚æ•°æ§åˆ¶ç”Ÿæˆè´¨é‡\n",
        "\n",
        "### 4.4 å‚æ•°è°ƒä¼˜\n",
        "- âœ… Temperature æ§åˆ¶éšæœºæ€§\n",
        "- âœ… é‡‡æ ·ç­–ç•¥å½±å“è¾“å‡ºè´¨é‡\n",
        "- âœ… æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ç­–ç•¥\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ef8585",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. å®è·µä»£ç \n",
        "\n",
        "### 5.1 ç¯å¢ƒå‡†å¤‡ä¸ä¾èµ–å®‰è£…\n",
        "\n",
        "åœ¨ä½¿ç”¨å¼€æºå¤§è¯­è¨€æ¨¡å‹ä¹‹å‰ï¼Œé¦–å…ˆéœ€è¦æ­å»ºåˆé€‚çš„å¼€å‘ç¯å¢ƒå¹¶å®‰è£…æ‰€éœ€çš„ä¾èµ–åº“ã€‚\n",
        "\n",
        "**æ­¥éª¤ 1ï¼šå®‰è£… PyTorch**\n",
        "\n",
        "æ ¹æ®ä½ çš„ç¡¬ä»¶ç¯å¢ƒï¼ˆCPU æˆ– GPUï¼‰é€‰æ‹©åˆé€‚çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼š\n",
        "\n",
        "- **CPU ç‰ˆæœ¬**ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47572e98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£… PyTorch (CPU ç‰ˆæœ¬)\n",
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ec91e92f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£… PyTorch (GPU ç‰ˆæœ¬ï¼ŒCUDA 11.8)\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09f6ffd",
      "metadata": {},
      "source": [
        "**æ­¥éª¤ 2ï¼šéªŒè¯å®‰è£…**\n",
        "\n",
        "å®‰è£…å®Œæˆåï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6b38765",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "Is CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# å¦‚æœ CUDA å¯ç”¨ï¼Œè¯´æ˜ä½ çš„ GPU ç¯å¢ƒå·²ç»é…ç½®æˆåŠŸï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹æ¨ç†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d7aba4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…/å‡çº§ transformers åº“\n",
        "# å¦‚æœé‡åˆ° GenerationMixin å¯¼å…¥é”™è¯¯ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å‡çº§ transformersï¼š\n",
        "!pip install --upgrade transformers>=4.33.0\n",
        "# æˆ–è€…ç›´æ¥å®‰è£…æœ€æ–°ç‰ˆæœ¬ï¼š\n",
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5ed46ffe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9664737cf4564d53898fde4d4913df66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Anaconda\\envs\\hands-on-aai\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Peiyan Li\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0399927d80242d8a24be4409ac64a3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cde10aeb6b84f7e9d4f60b9be19a544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9510a3be5beb4eefbd58ca91dac20357",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce365cae4a7f41d99351ba2489e35f13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5432154614ce4774b1e24aec4bf75333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "884ffa49cfd64503ac4d97b39f8952c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# æŒ‡å®šæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„\n",
        "# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œå¦‚æœä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°å³å¯\n",
        "# ä¾‹å¦‚ï¼šmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è·¯å¾„\n",
        "\n",
        "# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# å°†æ¨¡å‹ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5f419aa6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total model parameters: 494.03M\n",
            "Qwen2ForCausalLM(\n",
            "  (model): Qwen2Model(\n",
            "    (embed_tokens): Embedding(151936, 896)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x Qwen2DecoderLayer(\n",
            "        (self_attn): Qwen2Attention(\n",
            "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
            "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
            "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
            "        )\n",
            "        (mlp): Qwen2MLP(\n",
            "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
            "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
            "    (rotary_emb): Qwen2RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# æŸ¥çœ‹æ¨¡å‹å‚æ•°é‡\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total model parameters: {total_params / 1e6:.2f}M\")\n",
        "\n",
        "# æŸ¥çœ‹æ¨¡å‹ç»“æ„ï¼ˆå¯é€‰ï¼Œè¾“å‡ºè¾ƒé•¿ï¼‰\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d7d5ca",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5.3 æ–‡å­—ç»­å†™ä»»åŠ¡\n",
        "\n",
        "æ–‡å­—ç»­å†™æ˜¯ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½ä¹‹ä¸€ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„ä»£ç ç¤ºä¾‹ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97ee8ec6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ç”Ÿæˆçš„æ–‡æœ¬ï¼š\n",
            "ä»å‰æœ‰ä¸€ä¸ªå°æ‘åº„ï¼Œæ‘é‡Œä½ç€ä¸€ä½èªæ˜çš„è€äººã€‚æœ‰ä¸€å¤©ï¼Œæ‘é‡Œçš„å­©å­éƒ½é—®ä»–ä¸€ä¸ªé—®é¢˜ï¼šâ€œä½ æ˜¯ä»€ä¹ˆï¼Ÿâ€\n",
            "è€äººä»¬è¯´ï¼šâ€œæˆ‘æ˜¯è°ï¼Ÿâ€\n",
            "å­©å­ä»¬ç–‘æƒ‘åœ°é—®é“ï¼šâ€œæˆ‘ä»¬éƒ½æ˜¯è°ï¼Ÿâ€\n",
            "\n",
            "è€äººç»§ç»­è¯´é“ï¼šâ€œæˆ‘å°±æ˜¯ä½ æ‰€è¯´çš„â€˜æˆ‘æ˜¯è°â€™ã€‚â€\n",
            "å­©å­ä»¬çš„å›°æƒ‘å’Œä¸è§£è®©ä»–æ›´åŠ æ„Ÿåˆ°æ— å¥ˆã€‚\n",
            "äºæ˜¯ï¼Œä»–å¼€å§‹å‘å­©å­ä»¬è®²è¿°ä»–çš„æ•…äº‹ï¼Œå‘Šè¯‰ä»–ä»¬è‡ªå·±çš„åå­—ã€å®¶åº­ä½å€ä»¥åŠä»–çš„äººç”Ÿç»å†ã€‚\n",
            "å­©å­ä»¬è¢«è¿™ä¸ªå……æ»¡æ™ºæ…§çš„æ•…äº‹æ·±æ·±å¸å¼•ï¼Œçº·çº·è¯¢é—®ä»–æ›´å¤šçš„ä¿¡æ¯ã€‚\n",
            "\n",
            "ç»è¿‡ä¸€æ®µæ—¶é—´åï¼Œå­©å­ä»¬é€æ¸å¯¹è¿™ä½è€äººç”Ÿå‡ºäº†æ•¬ä½©ä¹‹æƒ…ã€‚åœ¨ä¸€æ¬¡å¤–å‡ºæ¸¸ç©æ—¶ï¼Œå­©å­ä»¬å‘ç°äº†ä¸€ä¸ªç¥ç§˜çš„çŸ³ç¢‘ï¼Œä¸Šé¢åˆ»æœ‰â€œæˆ‘æ˜¯è°â€çš„å­—æ ·ã€‚ä»–ä»¬å¸¦ç€ç–‘é—®å»æ‹œè®¿äº†é‚£ä½è€äººï¼Œè€äººä¹Ÿè€å¿ƒåœ°å›ç­”äº†ä»–ä»¬çš„é—®é¢˜ã€‚\n",
            "\n",
            "æœ€ç»ˆï¼Œåœ¨ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ—¥å­é‡Œï¼Œå­©å­ä»¬å†³å®šé‚€è¯·è¿™ä½è€äººå›å®¶è¿‡å¤œï¼Œä¸€èµ·åˆ†äº«ä»–ä»¬å¿ƒä¸­çš„ç§˜å¯†ã€‚\n",
            "\n",
            "ä»æ­¤ä»¥åï¼Œæ¯å½“å­©å­ä»¬é‡åˆ°å›°éš¾æˆ–éœ€è¦å¸®åŠ©æ—¶ï¼Œä»–ä»¬éƒ½ä¼šæƒ³èµ·é‚£ä¸ªå……æ»¡æ™ºæ…§çš„è€äººå’Œä»–çš„æ•…äº‹ã€‚è€Œè¿™ä½è€äººä¹Ÿæˆä¸ºäº†æ‘é‡Œæµä¼ æœ€å¹¿çš„è‹±é›„äººç‰©ä¹‹ä¸€ã€‚ä»–çš„æ•…äº‹æ¿€åŠ±ç€æ¯ä¸€ä¸ªå­©å­ï¼Œè®©ä»–ä»¬çŸ¥é“åªè¦å‹‡äºæ¢ç´¢æœªçŸ¥ï¼Œå°±ä¸€å®šèƒ½å¤Ÿæ‰¾åˆ°å±äºè‡ªå·±çš„å…‰èŠ’ã€‚\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device  # æŒ‡å®šè®¾å¤‡\n",
        ")\n",
        "\n",
        "# è¾“å…¥æç¤ºæ–‡æœ¬\n",
        "prompt = \"ä»å‰æœ‰ä¸€ä¸ªå°æ‘åº„ï¼Œæ‘é‡Œä½ç€ä¸€ä½èªæ˜çš„è€äººã€‚\"\n",
        "\n",
        "# ç”Ÿæˆç»­å†™æ–‡æœ¬\n",
        "generated_text = text_generator(\n",
        "    prompt,\n",
        "    max_length=200,  # æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
        "    min_length=50,   # æœ€å°ç”Ÿæˆé•¿åº¦\n",
        "    do_sample=True,  # æ˜¯å¦ä½¿ç”¨é‡‡æ ·\n",
        "    early_stopping=True  # æå‰åœæ­¢ç”Ÿæˆ\n",
        ")[0]['generated_text']\n",
        "\n",
        "print(\"ç”Ÿæˆçš„æ–‡æœ¬ï¼š\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608a0ef9",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### 5.4 ç”Ÿæˆè¿‡ç¨‹çš„å¹²é¢„å®è·µ\n",
        "\n",
        "åœ¨ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°æ¥å¹²é¢„ç”Ÿæˆç»“æœçš„é£æ ¼å’Œè´¨é‡ã€‚è¿™äº›å‚æ•°ä¸»è¦åŒ…æ‹¬æ¸©åº¦ï¼ˆTemperatureï¼‰å’Œé‡‡æ ·ç­–ç•¥ã€‚\n",
        "\n",
        "**ç¤ºä¾‹ï¼šå¯¹æ¯”ä¸åŒå‚æ•°è®¾ç½®ä¸‹çš„ç”Ÿæˆæ•ˆæœ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a991f158",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "ä¸¥è°¨å›ç­”ï¼š\n",
            " å¤§è§„æ¨¡é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPTã€BERTç­‰ï¼Œåœ¨å¤„ç†æ–‡æœ¬æ—¶ä¼šä½¿ç”¨ä¸€ç§ç§°ä¸ºâ€œæ³¨æ„åŠ›æœºåˆ¶â€çš„æŠ€æœ¯æ¥å¢å¼ºå…¶æ€§èƒ½å’Œç†è§£èƒ½åŠ›ã€‚\n",
            "\n",
            "åœ¨ä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å…³æ³¨çš„æ˜¯å¦‚ä½•è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›æœ€å¤§åŒ–ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œäººä»¬å‘ç°é€šè¿‡å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ˜¾è‘—æé«˜æœºå™¨å¯¹è¾“å…¥ä¿¡æ¯çš„ç†è§£èƒ½åŠ›å’Œè¡¨è¾¾è´¨é‡ã€‚\n",
            "\n",
            "### æ³¨æ„åŠ›æœºåˆ¶çš„åŸºæœ¬æ¦‚å¿µ\n",
            "\n",
            "1. **æ³¨æ„åŠ›è¿‡ç¨‹**ï¼šè¿™æ˜¯æŒ‡ä¸€ä¸ªç‰¹å®šä»»åŠ¡ï¼ˆä¾‹å¦‚è¯†åˆ«æŸä¸ªå•è¯æˆ–å¥å­ï¼‰ä¸å¦ä¸€ä¸ªéƒ¨åˆ†ä¹‹é—´çš„äº¤äº’ä½œç”¨ã€‚\n",
            "2. **æƒé‡åˆ†é…**ï¼šæ¯ä¸ªä½ç½®ä¸Šçš„è¯è¢«èµ‹äºˆä¸åŒçš„æƒé‡ï¼Œè¿™äº›æƒé‡åæ˜ äº†è¯¥ä½ç½®ä¸Šè¯æ±‡çš„é‡è¦æ€§ã€‚\n",
            "3. **å±€éƒ¨è§†å›¾**ï¼šåœ¨ä¸€ä¸ªç»™å®šçš„ä½ç½®ä¸Šï¼Œæ‰€æœ‰å¯èƒ½å½±å“å½“å‰ä¸Šä¸‹æ–‡çš„ä¿¡æ¯éƒ½è¢«è€ƒè™‘è¿›æ¥ï¼Œå¹¶ä¸”å®ƒä»¬ä¼šè¢«åŠ æƒå¹¶è¿›è¡Œç»„åˆä»¥å½¢æˆæœ€ç»ˆçš„ç»“æœã€‚\n",
            "\n",
            "### ä¸ºä»€ä¹ˆéœ€è¦æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ\n",
            "\n",
            "- **æå‡ç†è§£æ•ˆç‡**: åœ¨å¤§è§„æ¨¡æ•°æ®é›†çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œç”±äºæ ·æœ¬æ•°é‡åºå¤§ï¼Œä¼ ç»Ÿæ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ•æ‰å…¨å±€ç‰¹å¾ã€‚è€Œæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°èšç„¦äºå…³é”®ç‚¹ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°å®Œæˆå¤æ‚çš„åˆ†æä»»åŠ¡ã€‚\n",
            "  \n",
            "- **å‡å°‘è¿‡æ‹Ÿåˆé£é™©**: å½“å‰è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨è¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ï¼Œå³æ¨¡å‹å®¹æ˜“\n",
            "\n",
            "åˆ›æ„å›ç­”ï¼š\n",
            " ä¸€èˆ¬è€Œè¨€ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTå’ŒGPTï¼‰å¹¶æ²¡æœ‰åœ¨ä¼ ç»Ÿä¸Šä¸‹æ–‡ä¸­ç†è§£é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œåœ¨è¯¢é—®æœºå™¨ä¸­å›ç­”çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œå› æ­¤ï¼Œåœ¨è¿™äº›æ¨¡å‹ä¸­å¹¶æ²¡æœ‰å¤„ç†æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬ä»…ä»…å…·æœ‰å¤„ç†å…¶ä»–ä¿¡æ¯ï¼Œå¦‚è¯­è¨€æ¨¡å‹å’Œæ•°æ®ç›¸å…³é—®é¢˜çš„æ€§èƒ½ã€‚\n",
            "\n",
            "ä½†æ˜¯ï¼Œæœ‰è®¸å¤šç°ä»£çš„æœºå™¨å­¦ä¹ æ–¹æ³•å’ŒæŠ€æœ¯å¯èƒ½ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾ç¥ç»ç½‘ç»œå’ŒLSTMç¥ç»ç½‘ç»œä¸Šï¼Œé€šè¿‡åœ¨ç‰¹å®šéƒ¨åˆ†æˆ–å¯¹ç‰¹å®šä½ç½®çš„è¾“å…¥ä¸Šåº”ç”¨è‡ªå›å½’è®­ç»ƒï¼Œå¯ä»¥åœ¨è¿™äº›æ¨¡å‹ä¸­å¢å¼ºè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¯é¢‘èšç±»çš„é—®é¢˜å’Œå¯¹å¤šè¯å¥çš„è¯†åˆ«æˆ–å‘½åæ—¶ã€‚è¿™ç§è®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¸éœ€è¦æ³¨æ„ä¹‹å‰çš„ä¿¡æ¯çš„å®Œæ•´æ€§ï¼Œå¹¶ä¸”ä¼šè€ƒè™‘åˆ°æ‰€æœ‰ç›¸é‚»è¾“å…¥çš„å¯èƒ½æ€§ã€‚\n",
            "\n",
            "æ­¤å¤–ï¼Œæœ‰äº›æœºå™¨å­¦ä¹ æ–¹æ³•åŒ…æ‹¬å¤šæ¨¡æ€å’Œæ··åˆæ¨¡æ€ï¼Œå®ƒä»¬èƒ½å¤Ÿå¤„ç†ç”±ä¸åŒæ¨¡æ€è¾“å…¥çš„æƒ…å¢ƒå’Œæƒ…æ™¯ã€‚å°½ç®¡è¿™äº›å­¦ä¹ è¿‡ç¨‹ç›®å‰æ²¡æœ‰æä¾›ç›´æ¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†å®ƒå¯èƒ½è¢«ç”¨æ¥å¢å¼ºæœºå™¨çš„å¤šæ¨¡æ€èƒ½åŠ›å’Œç†è§£é—®é¢˜ã€‚\n",
            "\n",
            "æ€»ä¹‹ï¼Œå°½ç®¡ç°ä»£çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚BERTå’ŒGPTï¼‰å¯ä»¥å¤„ç†ç”±å…·ä½“è¯­è¨€è¡¨è¾¾ã€å¥å­å’Œå¤šè¯­æ–™æè¿°çš„ä¿¡æ¯ï¼Œæ²¡æœ‰ä¸“é—¨çš„æ³¨æ„åŠ›æœºåˆ¶æ¥é’ˆå¯¹è¿™äº›ç‰¹å®šé—®é¢˜ï¼Œä½†æŸäº›é¢†åŸŸçš„æœºå™¨å­¦ä¹ ç ”ç©¶æ­£åœ¨æ¢ç´¢å¦‚ä½•ä½¿ç”¨è¯¥æŠ€æœ¯\n"
          ]
        }
      ],
      "source": [
        "# è¾“å…¥æç¤ºæ–‡æœ¬\n",
        "prompt =  \"è¯·ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚\"\n",
        "\n",
        "# ç”Ÿæˆä¸¥è°¨å›ç­”ï¼ˆä½æ¸©å€¼ï¼Œè´ªå¿ƒé‡‡æ ·ï¼‰\n",
        "rigorous_text = text_generator(\n",
        "    prompt,\n",
        "    max_length=200,\n",
        "    temperature=0.1,  # ä½æ¸©å€¼\n",
        "    top_k=10,          # Top-k é‡‡æ ·ï¼Œä½kå€¼\n",
        "    top_p=0.5,        # Top-p é‡‡æ ·ï¼Œ ä½på€¼\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.2  # é¿å…é‡å¤\n",
        ")[0]['generated_text']\n",
        "\n",
        "print(\"-\"*50)\n",
        "\n",
        "print(\"ä¸¥è°¨å›ç­”ï¼š\")\n",
        "print(rigorous_text[len(prompt):])  # å»æ‰æç¤ºæ–‡æœ¬éƒ¨åˆ†\n",
        "\n",
        "# ç”Ÿæˆåˆ›æ„å›ç­”ï¼ˆé«˜æ¸©å€¼ï¼ŒTop-p é‡‡æ ·ï¼‰\n",
        "creative_text = text_generator(\n",
        "    prompt,\n",
        "    max_length=200,\n",
        "    temperature=1.5,  # é«˜æ¸©å€¼\n",
        "    top_k=50,         # Top-k é‡‡æ ·ï¼Œ é«˜kå€¼\n",
        "    top_p=0.9,        # Top-p é‡‡æ ·ï¼Œé«˜på€¼\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.0  # å…è®¸ä¸€å®šé‡å¤\n",
        ")[0]['generated_text'][len(prompt):]  # å»æ‰æç¤ºæ–‡æœ¬éƒ¨åˆ†\n",
        "\n",
        "print(\"\\nåˆ›æ„å›ç­”ï¼š\")\n",
        "print(creative_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10bec73",
      "metadata": {},
      "source": [
        "å¯ä»¥çœ‹åˆ°ï¼Œä¸¥è°¨å›ç­”åˆ†ç‚¹æ¸…æ™°ï¼Œæ˜¾å¾—ä¸“ä¸šå¯ä¿¡ï¼›åˆ›æ„å›ç­”æ›´åå‘å¹³é“ºç›´å™ï¼Œæ˜¾å¾—é€šä¿—æ˜“æ‡‚ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### 5.5 æœ¬åœ°åŒ–æ™ºèƒ½ä½“\n",
        "\n",
        "åŸºäºå¼€æºæ¨¡å‹ï¼Œå¯ä»¥æ„å»ºæœ¬åœ°åŒ–çš„æ™ºèƒ½ä½“ï¼Œç”¨äºå•è½®æˆ–å¤šè½®å¯¹è¯ã€‚\n",
        "\n",
        "#### 5.5.1 å•è½®å¯¹è¯ï¼šé—®ç­”åŠ©æ‰‹\n",
        "\n",
        "å•è½®å¯¹è¯ç³»ç»Ÿç”¨äºå›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œé€‚åˆç®€å•çš„é—®ç­”åœºæ™¯ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d797910",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\n",
            "å›ç­”ï¼šäººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯æŒ‡ç”±è®¡ç®—æœºç³»ç»Ÿæ‰€è¡¨ç°å‡ºçš„æ™ºèƒ½è¡Œä¸ºæˆ–åŠŸèƒ½ã€‚å®ƒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œå¹¶ä¸”æ˜¯ç ”ç©¶å’Œå¼€å‘ç”¨äºæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººç±»æ™ºèƒ½çš„ä¸€é—¨å­¦ç§‘ã€‚\n",
            "\n",
            "äººå·¥æ™ºèƒ½åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€åŒ»ç–—è¯Šæ–­ã€é‡‘èåˆ†æç­‰é¢†åŸŸã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œäººå·¥æ™ºèƒ½æ­£é€æ¸æ¸—é€åˆ°æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­ï¼Œæˆä¸ºæˆ‘ä»¬ç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½ä¹Ÿé¢ä¸´ç€ä¸€äº›æŒ‘æˆ˜å’Œäº‰è®®ï¼Œä¾‹å¦‚éšç§ä¿æŠ¤ã€ä¼¦ç†é“å¾·ç­‰é—®é¢˜éœ€è¦æˆ‘ä»¬åœ¨ä½¿ç”¨äººå·¥æ™ºèƒ½æ—¶è¿›è¡Œæ·±å…¥æ€è€ƒå’Œè®¨è®ºã€‚æ€»çš„æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå……æ»¡æ´»åŠ›å’Œæ½œåŠ›çš„é¢†åŸŸï¼Œä½†åŒæ—¶ä¹Ÿéœ€è¦æˆ‘ä»¬è°¨æ…å¯¹å¾…å…¶å‘å±•å¸¦æ¥çš„å½±å“ã€‚\n"
          ]
        }
      ],
      "source": [
        "def single_turn_qa(question):\n",
        "    # è¾“å…¥é—®é¢˜\n",
        "    prompt = f\"é—®é¢˜ï¼š{question}\\nå›ç­”ï¼š\"\n",
        "    \n",
        "    # ç”Ÿæˆå›ç­”\n",
        "    response = text_generator(\n",
        "        prompt,\n",
        "        max_length=300,\n",
        "        temperature=0.5,  # ä¸­ç­‰æ¸©åº¦å€¼\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        num_return_sequences=1,\n",
        "        repetition_penalty=1.1\n",
        "    )[0]['generated_text'].split(\"å›ç­”ï¼š\")[1]\n",
        "    \n",
        "    return response\n",
        "\n",
        "# æµ‹è¯•å•è½®é—®ç­”\n",
        "question = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\"\n",
        "answer = single_turn_qa(question)\n",
        "print(f\"é—®é¢˜ï¼š{question}\")\n",
        "print(f\"å›ç­”ï¼š{answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11154015",
      "metadata": {},
      "source": [
        "#### 5.5.2 å¤šè½®å¯¹è¯ï¼šèŠå¤©æœºå™¨äºº\n",
        "\n",
        "å¤šè½®å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿç»´æŠ¤å¯¹è¯å†å²ï¼Œä½¿å¯¹è¯æ›´å…·è¿è´¯æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f0fdf05e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------å¯¹è¯1----------------\n",
            "ç”¨æˆ·ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚\n",
            "åŠ©æ‰‹ï¼šæˆ‘è®¤ä¸ºï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œäººå·¥æ™ºèƒ½å°†ä¼šå˜å¾—æ›´åŠ æ™ºèƒ½åŒ–ã€è‡ªä¸»åŒ–å’Œäººæ€§åŒ–ã€‚å®ƒå°†æ›´åŠ æ“…é•¿å¤„ç†å¤æ‚çš„ä»»åŠ¡ï¼Œç”šè‡³æœ‰å¯èƒ½å®ç°è‡ªæˆ‘å­¦ä¹ å’Œè‡ªæˆ‘æ”¹è¿›\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------å¯¹è¯2----------------\n",
            "ç”¨æˆ·ï¼šåˆ—ä¸¾å®ƒçš„åº”ç”¨åœºæ™¯ã€‚\n",
            "åŠ©æ‰‹ï¼šæˆ‘è®¤ä¸ºï¼Œäººå·¥æ™ºèƒ½å°†åœ¨æœªæ¥çš„å‡ å¹´å†…å–å¾—æ›´å¤§çš„å‘å±•ï¼Œå¹¶ä¸”ä¼šæœç€æ›´å¹¿æ³›çš„åº”ç”¨é¢†åŸŸæ‹“å±•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…çœ‹åˆ°æ›´å¤šçš„æœºå™¨äººã€æ™ºèƒ½éŸ³ç®±ã€æ™ºèƒ½å®¶å±…è®¾å¤‡ç­‰äº§å“å‡ºç°ï¼›æˆ‘ä»¬ä¹Ÿå¯ä»¥é¢„æœŸåˆ°æ›´å¤šçš„äººå·¥æ™ºèƒ½åº”ç”¨å°†è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥æé«˜å·¥ä½œæ•ˆç‡å’Œç”Ÿæ´»è´¨é‡ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å…³æ³¨å¦‚ä½•ç¡®ä¿äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œé˜²æ­¢å…¶æ»¥ç”¨æˆ–è¯¯ç”¨ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘è®¤ä¸ºäººå·¥æ™ºèƒ½æ˜¯æœªæ¥ç§‘æŠ€å‘å±•çš„ä¸»è¦æ–¹å‘ä¹‹ä¸€ï¼Œå…·æœ‰å·¨å¤§çš„å‘å±•æ½œåŠ›å’Œæ½œåŠ›ã€‚\n",
            "----------------å¯¹è¯3----------------\n",
            "ç”¨æˆ·ï¼šä½¿ç”¨äººå·¥æ™ºèƒ½éœ€è¦æ³¨æ„ä»€ä¹ˆã€‚\n",
            "åŠ©æ‰‹ï¼šæˆ‘å»ºè®®åœ¨ä½¿ç”¨äººå·¥æ™ºèƒ½æ—¶è¦éµå®ˆç›¸å…³çš„æ³•å¾‹æ³•è§„ï¼Œä¿æŠ¤ä¸ªäººéšç§ï¼Œé¿å…è¿‡åº¦ä¾èµ–äººå·¥æ™ºèƒ½è€Œå¿½è§†äº†äººçš„ä»·å€¼å’Œæ„ä¹‰ã€‚æ­¤å¤–ï¼Œä¹Ÿè¦æ³¨é‡äººå·¥æ™ºèƒ½çš„å‘å±•è¿‡ç¨‹ä¸­çš„ä¼¦ç†é—®é¢˜ï¼Œç¡®ä¿å…¶å¯¹ç¤¾ä¼šçš„å½±å“æ˜¯ç§¯ææœ‰ç›Šçš„ã€‚æœ€åï¼Œæˆ‘ä»¬åº”è¯¥é¼“åŠ±å’Œæ”¯æŒäººå·¥æ™ºèƒ½çš„ç ”ç©¶å’Œå‘å±•ï¼Œè®©å…¶ä¸ºäººç±»å¸¦æ¥æ›´å¤šçš„ä¾¿åˆ©å’Œç¦ç¥‰ã€‚æ€»ä¹‹ï¼Œæˆ‘è®¤ä¸ºäººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªæ—¢å……æ»¡æœºé‡åˆé¢ä¸´æŒ‘æˆ˜çš„é‡è¦é¢†åŸŸï¼Œæˆ‘ä»¬éœ€è¦åšå¥½å……åˆ†å‡†å¤‡å¹¶åŠ ä»¥åˆ©ç”¨ã€‚\n"
          ]
        }
      ],
      "source": [
        "class MultiTurnChatbot:\n",
        "    def __init__(self):\n",
        "        self.history = []  # å­˜å‚¨å¯¹è¯å†å²\n",
        "    \n",
        "    def generate_response(self, user_input):\n",
        "        # æ„å»ºåŒ…å«å†å²çš„æç¤º\n",
        "        prompt = \"å¯¹è¯å†å²ï¼š\\n\"\n",
        "        for turn in self.history:\n",
        "            prompt += f\"ç”¨æˆ·ï¼š{turn['user']}\\nåŠ©æ‰‹ï¼š{turn['assistant']}\\n\"\n",
        "        prompt += f\"ç”¨æˆ·ï¼š{user_input}\\nåŠ©æ‰‹ï¼š\"\n",
        "        \n",
        "        # ç”Ÿæˆå›ç­”\n",
        "        response = text_generator(\n",
        "            prompt,\n",
        "            max_length=300,\n",
        "            temperature=0.5,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            num_return_sequences=1,\n",
        "            repetition_penalty=1.1\n",
        "        )[0]['generated_text'].split(\"åŠ©æ‰‹ï¼š\")[-1]\n",
        "        \n",
        "        # ä¿å­˜å½“å‰å¯¹è¯åˆ°å†å²\n",
        "        self.history.append({\"user\": user_input, \"assistant\": response})\n",
        "        \n",
        "        return response\n",
        "\n",
        "# æµ‹è¯•å¤šè½®å¯¹è¯\n",
        "chatbot = MultiTurnChatbot()\n",
        "\n",
        "# ç¬¬ä¸€è½®å¯¹è¯\n",
        "user_input = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚\"\n",
        "response = chatbot.generate_response(user_input)\n",
        "print(\"----------------å¯¹è¯1----------------\")\n",
        "print(f\"ç”¨æˆ·ï¼š{user_input}\")\n",
        "print(f\"åŠ©æ‰‹ï¼š{response}\")\n",
        "\n",
        "# ç¬¬äºŒè½®å¯¹è¯\n",
        "user_input = \"åˆ—ä¸¾å®ƒçš„åº”ç”¨åœºæ™¯ã€‚\"\n",
        "response = chatbot.generate_response(user_input)\n",
        "print(\"----------------å¯¹è¯2----------------\")\n",
        "print(f\"ç”¨æˆ·ï¼š{user_input}\")\n",
        "print(f\"åŠ©æ‰‹ï¼š{response}\")\n",
        "\n",
        "# ç¬¬ä¸‰è½®å¯¹è¯\n",
        "user_input = \"ä½¿ç”¨äººå·¥æ™ºèƒ½éœ€è¦æ³¨æ„ä»€ä¹ˆã€‚\"\n",
        "response = chatbot.generate_response(user_input)\n",
        "print(\"----------------å¯¹è¯3----------------\")\n",
        "print(f\"ç”¨æˆ·ï¼š{user_input}\")\n",
        "print(f\"åŠ©æ‰‹ï¼š{response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd71507",
      "metadata": {},
      "source": [
        "> ğŸ’¡ **æç¤º**ï¼šåœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œå¯¹è¯å†å²çš„ç»´æŠ¤æ˜¯å…³é”®ã€‚é€šè¿‡å°†å¤šè½®å¯¹è¯æ‹¼æ¥ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¹‹å‰çš„å¯¹è¯å†…å®¹ç”Ÿæˆæ›´è¿è´¯çš„å›ç­”ã€‚ä½ å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´å¯¹è¯å†å²çš„é•¿åº¦ï¼Œä»¥å¹³è¡¡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œå¯¹è¯è¿è´¯æ€§ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## 6. ä»£ç è¦ç‚¹æ€»ç»“\n",
        "\n",
        "### 6.1 ç¯å¢ƒé…ç½®\n",
        "- âœ… å®‰è£… PyTorchï¼ˆæ ¹æ®ç¡¬ä»¶é€‰æ‹© CPU æˆ– GPU ç‰ˆæœ¬ï¼‰\n",
        "- âœ… å®‰è£… Transformers åº“\n",
        "- âœ… éªŒè¯ CUDA æ˜¯å¦å¯ç”¨ï¼ˆGPU åŠ é€Ÿï¼‰\n",
        "\n",
        "### 6.2 æ¨¡å‹åŠ è½½\n",
        "- âœ… ä½¿ç”¨ `AutoTokenizer` åŠ è½½åˆ†è¯å™¨\n",
        "- âœ… ä½¿ç”¨ `AutoModelForCausalLM` åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
        "- âœ… å°†æ¨¡å‹ç§»åŠ¨åˆ°åˆé€‚çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰\n",
        "\n",
        "### 6.3 æ–‡æœ¬ç”Ÿæˆ\n",
        "- âœ… ä½¿ç”¨ `pipeline` åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“\n",
        "- âœ… é€šè¿‡ `max_length`ã€`min_length` æ§åˆ¶ç”Ÿæˆé•¿åº¦\n",
        "- âœ… é€šè¿‡ `temperature`ã€`top_k`ã€`top_p` æ§åˆ¶ç”Ÿæˆè´¨é‡\n",
        "\n",
        "### 6.4 å‚æ•°è°ƒä¼˜\n",
        "- âœ… **ä¸¥è°¨å›ç­”**ï¼š`temperature=0.1`, `top_k=10`, `top_p=0.5`\n",
        "- âœ… **åˆ›æ„å›ç­”**ï¼š`temperature=1.5`, `top_k=50`, `top_p=0.9`\n",
        "- âœ… **å¹³è¡¡å›ç­”**ï¼š`temperature=0.5-0.7`, `top_p=0.9`\n",
        "\n",
        "### 6.5 æ™ºèƒ½ä½“æ„å»º\n",
        "- âœ… å•è½®é—®ç­”ï¼šç®€å•ç›´æ¥ï¼Œé€‚åˆä¸€æ¬¡æ€§é—®é¢˜\n",
        "- âœ… å¤šè½®å¯¹è¯ï¼šç»´æŠ¤å†å²ï¼Œæä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥\n",
        "\n",
        "---\n",
        "\n",
        "## 7. æ³¨æ„äº‹é¡¹\n",
        "\n",
        "1. **æ¨¡å‹è·¯å¾„**ï¼šç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–ä½¿ç”¨ Hugging Face Hub ä¸Šçš„æ¨¡å‹åç§°\n",
        "2. **å†…å­˜ç®¡ç†**ï¼šå¤§æ¨¡å‹éœ€è¦è¾ƒå¤šå†…å­˜ï¼Œæ³¨æ„ç›‘æ§ç³»ç»Ÿèµ„æº\n",
        "3. **å‚æ•°è°ƒä¼˜**ï¼šæ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼Œå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§\n",
        "4. **å¯¹è¯å†å²**ï¼šå¤šè½®å¯¹è¯ä¸­æ³¨æ„æ§åˆ¶å†å²é•¿åº¦ï¼Œé¿å…è¶…å‡ºæ¨¡å‹ä¸Šä¸‹æ–‡çª—å£\n",
        "\n",
        "---\n",
        "\n",
        "**ä¸‹ä¸€ç« **ï¼šæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­è°ƒç”¨å’Œä½¿ç”¨å¤§æ¨¡å‹ APIã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08f91de",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hands-on-aai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
