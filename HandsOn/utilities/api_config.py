"""
APIé…ç½®åŠ è½½å’Œæ™ºèƒ½ä½“å·¥å…·æ¨¡å—

æä¾›ä»JSONé…ç½®æ–‡ä»¶åŠ è½½APIé…ç½®çš„åŠŸèƒ½ï¼Œä»¥åŠåŸºäºOpenAI APIæ ¼å¼çš„é—®ç­”æ™ºèƒ½ä½“ç±»ã€‚

ä½¿ç”¨æ–¹æ³•:
    from HandsOn.utilities.api_config import load_config, QAgent, get_response
    
    # æ–¹å¼1: ä½¿ç”¨get_responseå‡½æ•°ï¼ˆæ¨èï¼Œä¸chapter_3.ipynbå…¼å®¹ï¼‰
    config = load_config()
    response = get_response(system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹", user_prompt="ä½ å¥½")
    
    # æ–¹å¼2: ä½¿ç”¨QAgentç±»
    agent = QAgent(platform="openai", config=config)
    answer = agent.ask("ä½ å¥½")
"""

import json
import os
from pathlib import Path
from openai import OpenAI


def load_config(config_path=None):
    """
    ä»JSONæ–‡ä»¶åŠ è½½APIé…ç½®
    è‡ªåŠ¨æŸ¥æ‰¾é…ç½®æ–‡ä»¶ï¼šä¼˜å…ˆæŸ¥æ‰¾ HandsOn/config.json
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
    
    Returns:
        dict: åŒ…å«æ‰€æœ‰APIé…ç½®çš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    # å¦‚æœæœªæŒ‡å®šè·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    if config_path is None:
        # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
        current_dir = Path(__file__).parent.parent
        hands_on_dir = current_dir
        
        # å¯èƒ½çš„é…ç½®æ–‡ä»¶ä½ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        possible_paths = [
            "config.json",  # å½“å‰ç›®å½•ï¼ˆå¦‚æœnotebookåœ¨HandsOnç›®å½•è¿è¡Œï¼‰
            str(hands_on_dir / "config.json"),  # HandsOnç›®å½•ä¸‹çš„config.json
            "HandsOn/config.json",  # HandsOnå­ç›®å½•
            "../HandsOn/config.json",  # ä¸Šçº§ç›®å½•çš„HandsOnæ–‡ä»¶å¤¹
            os.path.join(os.getcwd(), "HandsOn", "config.json"),  # ç»å¯¹è·¯å¾„
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                config_path = path
                break
        
        if config_path is None:
            print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶")
            print("   è¯·ç¡®ä¿åœ¨ HandsOn/ ç›®å½•ä¸‹å­˜åœ¨ config.json æ–‡ä»¶")
            print("   å¯ä»¥ä» HandsOn/config.json.example å¤åˆ¶å¹¶å¡«å†™ä½ çš„APIå¯†é’¥")
            return None
    
    try:
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„å¹¶è¯»å–
        config_path = os.path.abspath(config_path)
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {config_path}")
        print("   è¯·å¤åˆ¶ HandsOn/config.json.example ä¸º HandsOn/config.json å¹¶å¡«å†™ä½ çš„APIå¯†é’¥")
        return None
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSONæ ¼å¼é”™è¯¯ - {e}")
        print(f"   è¯·æ£€æŸ¥ {config_path} æ–‡ä»¶çš„JSONè¯­æ³•")
        return None


class QAgent:
    """
    é—®ç­”æ™ºèƒ½ä½“ç±»ï¼Œæ”¯æŒä»é…ç½®æ–‡ä»¶è¯»å–APIä¿¡æ¯
    å…¼å®¹ OpenAI API æ ¼å¼ï¼ˆOpenAIã€ChatGLMç­‰ï¼‰
    """
    def __init__(self, api_key=None, base_url=None, model=None, 
                 platform="openai", config=None, 
                 system_prompt="ä½ æ˜¯ä¸€ä¸ªå¾ˆèªæ˜çš„æ™ºèƒ½ä½“ã€‚ä½ ä¼šç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·æå‡ºçš„ä»»ä½•é—®é¢˜ã€‚"):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„å‚æ•°ï¼‰
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„å‚æ•°ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„å‚æ•°ï¼‰
            platform: å¹³å°åç§°ï¼Œç”¨äºä»configä¸­è¯»å–é…ç½®ï¼ˆå¯é€‰: "openai", "chatglm"ï¼‰
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ä»æ–‡ä»¶åŠ è½½ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰æ™ºèƒ½ä½“è§’è‰²å’Œè¡Œä¸º
        """
        # å¦‚æœæä¾›äº†configå‚æ•°ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä»æ–‡ä»¶åŠ è½½
        if config is None:
            config = load_config()
        
        # ä¼˜å…ˆä½¿ç”¨ç›´æ¥æä¾›çš„å‚æ•°ï¼Œå¦åˆ™ä»configä¸­è¯»å–
        if config:
            platform_config = config.get(platform, {})
            self.api_key = api_key or platform_config.get("api_key")
            self.base_url = base_url or platform_config.get("base_url")
            self.model = model or platform_config.get("model")
        else:
            # å¦‚æœæ²¡æœ‰configï¼Œå¿…é¡»æä¾›å‚æ•°
            if not all([api_key, base_url, model]):
                raise ValueError(
                    "âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å¯ç”¨ä¸”æœªæä¾›å®Œæ•´å‚æ•°ã€‚\n"
                    "   è¯·æä¾› api_keyã€base_url å’Œ model å‚æ•°ï¼Œ"
                    "   æˆ–åˆ›å»º HandsOn/config.json é…ç½®æ–‡ä»¶"
                )
            self.api_key = api_key
            self.base_url = base_url
            self.model = model
        
        # éªŒè¯å¿…è¦å‚æ•°
        if not self.api_key or not self.base_url or not self.model:
            raise ValueError(
                f"âŒ é”™è¯¯: æœªæ‰¾åˆ° {platform} çš„å®Œæ•´APIé…ç½®ã€‚\n"
                f"   è¯·æ£€æŸ¥ HandsOn/config.json æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n"
                f"   - api_key\n"
                f"   - base_url\n"
                f"   - model"
            )
        
        self.platform = platform
        self.system_prompt = system_prompt
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        
        print(f"âœ… æ™ºèƒ½ä½“åˆå§‹åŒ–æˆåŠŸ")
        print(f"   å¹³å°: {platform.upper()}")
        print(f"   æ¨¡å‹: {self.model}")
        print(f"   APIåœ°å€: {self.base_url}")

    def ask(self, question, temperature=0.7, max_tokens=2048):
        """
        æé—®æ¥å£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼Œ0-1ä¹‹é—´ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        
        Returns:
            str: æ™ºèƒ½ä½“çš„å›ç­”
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": question},
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=False
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"âŒ é”™è¯¯: {str(e)}"
    
    def update_system_prompt(self, system_prompt):
        """æ›´æ–°ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = system_prompt
        print("âœ… ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°")


# å…¨å±€é»˜è®¤agentå®ä¾‹ï¼ˆç”¨äºget_responseå‡½æ•°ï¼‰
_default_agent = None


def get_response(system_prompt="", user_prompt="", model=None, temperature=0.0, top_p=1.0, 
                 max_tokens=2048, platform="openai", config=None, agent=None):
    """
    é€šç”¨çš„APIè°ƒç”¨å‡½æ•°ï¼Œç”¨äºæç¤ºå·¥ç¨‹å®è·µ
    å‚è€ƒ chapter_3.ipynb ä¸­çš„å®ç°
    
    Args:
        system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        user_prompt: ç”¨æˆ·æç¤ºè¯ï¼ˆå¿…éœ€ï¼‰
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆæ§åˆ¶éšæœºæ€§ï¼Œ0-1ä¹‹é—´ï¼Œé»˜è®¤0.0ï¼‰
        top_p: nucleus samplingå‚æ•°ï¼ˆé»˜è®¤1.0ï¼‰
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆé»˜è®¤2048ï¼‰
        platform: å¹³å°åç§°ï¼Œç”¨äºä»configä¸­è¯»å–é…ç½®ï¼ˆé»˜è®¤"openai"ï¼‰
        config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ä»æ–‡ä»¶åŠ è½½ï¼‰
        agent: QAgentå®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨å…¶clientå’Œé…ç½®ï¼‰
    
    Returns:
        str: æ¨¡å‹çš„å›ç­”
    """
    global _default_agent
    
    if not user_prompt:
        return "âŒ é”™è¯¯: ç”¨æˆ·æç¤ºè¯ä¸èƒ½ä¸ºç©º"
    
    # å¦‚æœæä¾›äº†agentï¼Œä½¿ç”¨å…¶clientå’Œé…ç½®
    if agent is not None:
        client = agent.client
        use_model = model or agent.model
    else:
        # å¦‚æœæ²¡æœ‰é»˜è®¤agentï¼Œåˆ›å»ºä¸€ä¸ª
        if _default_agent is None:
            if config is None:
                config = load_config()
            _default_agent = QAgent(platform=platform, config=config)
        client = _default_agent.client
        use_model = model or _default_agent.model
    
    # ä½¿ç”¨ä¼ å…¥çš„system_promptï¼ˆå³ä½¿æ˜¯ç©ºå­—ç¬¦ä¸²ä¹Ÿä½¿ç”¨ï¼‰
    # å‚è€ƒchapter_3.ipynbçš„å®ç°ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„system_prompt
    use_system_prompt = system_prompt
    
    try:
        # è°ƒç”¨APIï¼ˆå‚è€ƒchapter_3.ipynbçš„å®ç°ï¼‰
        response = client.chat.completions.create(
            model=use_model,
            messages=[
                {"role": "system", "content": use_system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}"


def set_default_agent(agent):
    """
    è®¾ç½®é»˜è®¤çš„agentå®ä¾‹ï¼Œä¾›get_responseå‡½æ•°ä½¿ç”¨
    
    Args:
        agent: QAgentå®ä¾‹
    """
    global _default_agent
    _default_agent = agent


# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œè¿›è¡Œæµ‹è¯•
if __name__ == "__main__":
    # åŠ è½½é…ç½®
    config = load_config()
    
    # æŸ¥çœ‹å·²é…ç½®çš„APIå¹³å°ï¼ˆä¸æ˜¾ç¤ºå¯†é’¥ï¼‰
    if config:
        print("\nğŸ“‹ å·²é…ç½®çš„APIå¹³å°:")
        for platform in config.keys():
            model_name = config[platform].get('model', 'N/A')
            base_url = config[platform].get('base_url', 'N/A')
            has_key = bool(config[platform].get('api_key'))
            status = "âœ…" if has_key else "âš ï¸"
            print(f"  {status} {platform.upper()}: {model_name} @ {base_url}")
