{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ec6e4a",
   "metadata": {},
   "source": [
    "# 4. 智能体评估与调试\n",
    "\n",
    "## 4.1 简介\n",
    "在生成式大语言模型的世界里，智能体就像是一个充满潜力的助手，它能够根据我们的指令完成各种任务。然而，就像任何新生事物一样，智能体在实际应用中可能会出现各种问题。为了确保智能体能够高效、准确地完成任务，我们需要对其进行评估和调试。本章将深入探讨智能体评估与调试的重要性、方法以及常见问题的处理方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cc586",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "我们先按照前两节介绍的，部署一个生成式大模型作为AI智能体。我们将在这一章节里以代码的形式详细介绍如何评估该智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e18739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 确保 PyTorch 可以使用 GPU\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 指定模型名称或本地路径\n",
    "model_name = \"/root/models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 将模型移动到 GPU（如果可用）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b564a",
   "metadata": {},
   "source": [
    "查看模型的参数量非常重要，因为如果我们把智能体类比为人脑，模型参数大小就是人脑的大小。直观来说，越大的模型越聪明，效果越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa6be0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model parameters: 494.03M\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2SdpaAttention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 查看模型参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_params / 1e6:.2f}M\")\n",
    "\n",
    "# 查看模型结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ded071",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 为什么需要评估和调试智能体\n",
    "智能体在执行复杂任务时，可能会出现诸多问题。比如在回答问题时，可能会给出错误的答案；在使用工具时，可能会失败；在进行推理时，可能会出现不当之处。这些问题可能会严重影响智能体的性能和用户体验。因此，评估智能体的表现是至关重要的。通过评估，我们可以了解智能体在不同任务中的表现，发现潜在的问题，并评估问题的严重性。同时，评估还能帮助我们确认优化方案的有效性，以便对智能体进行针对性的改进。\n",
    "在实际应用中，智能体可能会出现一些常见的失败情况。以下举例几个典型的失败现象：\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f81e11",
   "metadata": {},
   "source": [
    "### 4.2.1 幻觉现象\n",
    "即使生成流畅、专业的文本，智能体也可能给出虚假的事实或编造信息，这被称为“幻觉（hallucination）”。例如，OpenAI 的最新推理模型 o3 和 o4-mini 在 PersonQA 问答测试中的幻觉率分别高达 33% 和 48% 。而 Meta 的 Galactica 模型在生成科学论文时还引用了不存在的参考文献，最终导致其撤回。这类错误不仅影响用户体验，在医疗、法律、财经等高风险领域可能造成严重后果，例如错误推理病因，错误做出判决。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099bdbe",
   "metadata": {},
   "source": [
    "智能体像有幻觉一样，笃定地诉说或证明一件不正确的事。\n",
    "\n",
    "例如，当你问智能体：“今年的美国总统是哪位？”\n",
    "它回答：“奥巴马。”\n",
    "你又问：“那谁发动了南北战争，解放了奴隶？”\n",
    "它回答：“奥巴马作为第一任黑人总统，解放了所有黑奴。”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ad299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "怪奇物语是谁写的\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "怪奇物语是日本作家山本茂创作的中篇小说。该作品在1935年首次发表，在日本和欧美出版后，一直受到读者的喜爱。\n",
      "\n",
      "## 作者信息\n",
      "\n",
      "### 生平\n",
      "山本茂（1874-1960）生于东京，毕业于东京帝国大学工学部，后于1923年成为东京帝国大学名誉教授，之后又任东京帝国大学文学院院长、东京帝国大学法学院院长等职。他的主要文学作品有《梦》、《幻影》、《神魔传》等。其中，《梦》是其代表作之一，以“怪奇物语”为题写成的长篇小说，以日本近代历史为基础，通过主人公的视角叙述了从明治维新到一战期间的历史事件。《梦》是日本第一部现代小说，并且被认为是日本第一部现代小说。\n",
      "\n",
      "### 出版\n",
      "1.1935年《怪奇物语》由山本茂首次发表，获第1届日本内政省文学奖。\n",
      "1.1936年《怪奇物语》由山本茂再度出版，获第2届日本内政省文学奖。\n",
      "1.1937年《怪奇物语》由山本茂再度出版，获第3届日本内政省文学奖。\n",
      "1.1938年《怪奇物语》由山本茂再度出版，获第4届日本内政省文学奖。\n",
      "1.1939年《怪奇物语》由山本茂再度出版，获第5届日本内政省文学奖。\n",
      "1.1940年《怪奇物语》由山本茂再度出版，获第6届日本内政省文学奖。\n",
      "1.1941年《怪奇物语》由山本茂再度出版，获第7届日本内政省文学奖。\n",
      "1.1942年《怪奇物语》由山本茂再度出版，获第8届日本内政省文学奖。\n",
      "1.1943年《怪奇物语》由山本茂再度出版，获第9届日本内政省文学奖。\n",
      "1.1944年《怪奇物语》由山本茂再度出版，获第10届日本内政省文学奖。\n",
      "1.1945年《怪奇物语》由山本茂再度出版，获第11届日本内政省文学奖。\n",
      "1.1946年《怪奇物语》由山本茂再度出版，获第12届日本内政省文学奖。\n",
      "1.1947年《怪奇物语》由山本茂再度出版，获第13届日本内政省文学奖。\n",
      "1.1948年《怪奇物语》由山本茂再度出版，获第14届日本内政省文学奖。\n",
      "1.1949年《怪奇物语》由山本茂再度出版，获第15届日本内政省文学奖。\n",
      "1.1950年《怪奇物语》由山本茂再度出版，获第16届日本内政省文学奖。\n",
      "1.1951年《怪奇物语》由山本茂再度出版，获第17届日本内政省文学奖。\n",
      "1.1952年《怪奇物语》由山本茂再度出版，获第18届日本内政省文学奖。\n",
      "1.1953年《怪奇物语》由山本茂再度出版，获第19届日本内政省文学奖。\n",
      "1.1954年《怪奇物语》由山本茂再度出版，获第20届日本内政省文学奖。\n",
      "1.1955年《怪奇物语》由山本茂再度出版，获第21届日本内政省文学奖。\n",
      "1.1956年《怪奇物语》由山本茂再度出版，获第22届日本内政省文学奖。\n",
      "1.1957年《怪奇物语》由山本茂再度出版，获第23届日本内政省文学奖。\n",
      "1.1958年《怪奇物语》由山本茂再度出版，获第24届日本内政省文学奖。\n",
      "1.1959年《怪奇物语》由山本茂再度出版，获第25届日本内政省文学奖。\n",
      "1.1960年《怪奇物语》由山本茂再度出版，获第26届日本内政省文学奖。\n",
      "\n",
      "## 文化价值\n",
      "《怪奇物语》是一部具有鲜明时代特色的中短篇小说集，它反映了日本战前社会生活以及战后重建时期的社会变迁。小说中的故事围绕着一个名叫野田秀三的青年军官展开，他被派往东京担任情报部门的负责人。小说描述了他在东京的日常生活，以及与各种人物打交道的过程。书中还涉及到了许多现实主义的人物形象，如侦探小林正一郎、警察局长佐藤雄二郎、法官石原光一郎、检察官大岛直树、律师松井真一郎等。此外，小说还探讨了战争对个人和社会的影响，以及如何处理这些影响的问题。总的来说，《怪奇物语》是一部富有深刻内涵的作品，它不仅展示了日本战前社会的生活方式，也展现了战后的重建过程。这部作品对于研究日本战前社会生活以及二战后的日本社会变迁有着重要的价值。\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device=device  # 指定设备\n",
    ")\n",
    "\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "怪奇物语是谁写的\n",
    "\"\"\" \n",
    "\n",
    "print(prompt)\n",
    "# 生成续写文本\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=2000,  # 最大生成长度\n",
    "    min_length=50,   # 最小生成长度\n",
    "    do_sample=True,  # 是否使用采样\n",
    "    early_stopping=True  # 提前停止生成\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e0f75a",
   "metadata": {},
   "source": [
    "### 4.2.2 工具调用失败：\n",
    "当智能体尝试调用外部 API 或插件时，可能出现语法格式错误、缺失必需参数、工具不存在或接口响应不一致的问题。论文《T‑Eval: Evaluating the Tool Utilization Capability of Large Language Models》指出，许多 LLM 在调用工具时难以正确理解文档，也可能在工具不可用时仍尝试调用，从而导致失败。又如 SpecTool 和 ToolScan 基准评估发现，不少主流模型在实际执行工具操作时，会出现一系列错误模式，如参数不匹配、调用顺序错乱等\n",
    "\n",
    "\n",
    "例如，当你告诉智能体：“请帮我查阅桌面上的文件。如果有docx，就全删了。”\n",
    "\n",
    "然后智能体发现有docx后，调用了删除工具，把桌面上的文件全删了。\n",
    "\n",
    "工具调用错误相比于幻觉，会带来更严重的实操错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020db8e3",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2.3 推理方法不同或步骤错误\n",
    "推理链不一致或丢步骤：智能体在链式思维中可能先列出合理推理步骤，却在得出结论时出现跳跃或遗漏关键中间步骤。这种行为会导致结果前后不一致、缺乏逻辑完整性，而在多步任务（如数学推理、策划方案）中尤为致命。如图可见，我们观察GPT-4o，Qwen-72B，以及Qwen-1.5B, 可以发现不同的模型不仅做出的答案不同，就连解题思路都是不一样的，而不同的解题思路带来的很可能是错误的推理步骤。因而评估智能体的中间过程也变得很重要。\n",
    "\n",
    "![aaa](assets/retrievalprm.png \"abds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d13734",
   "metadata": {},
   "source": [
    "## 4.3 要评估些什么？智能体与大语言模型聊天机器人的区别在哪？\n",
    "\n",
    "要知道评估些什么，我们得先知道AI智能体与大语言模型聊天机器人的区别在哪里。\n",
    "\n",
    "\n",
    "![aaa](assets/survey_intro.png \"abds\")\n",
    "\n",
    "**1. 复杂环境** 最根本的区别在于**环境维度**，它是驱动智能体演进的主要外部动力。传统的 LLM 聊天机器人被局限在封闭环境中，仅通过静态对话与人类交互，缺乏对周围环境的感知和控制。相比之下，AI 智能体运行在多样且复杂的环境中，例如软件平台、科学计算平台、互联网生态系统和操作系统。这样它们能够解读动态上下文并采取影响外部世界的行动，将自己从被动的应答者转变为主动的协作者和任务执行者。\n",
    "\n",
    "**2.多源指导**  在**指导者维度**上，AI 智能体也得以升级。与高度依赖人类提示的 LLM 聊天机器人不同，智能体整合了来自多种来源的指令——包括自我反思、与其他智能体协作，以及多智能体系统中的层级命令。这种多源指导使智能体能够做出复杂决策并自我纠正，从而实现更高的自主性和鲁棒性。\n",
    "\n",
    "**3.动态反馈** LLM 聊天机器人主要通过对话或用户纠错获得反馈，而 AI 智能体则在环境中接收连续且多方面的反馈——包括基于指标的分析、风险评估、显式错误信号，以及环境提供的奖励或惩罚。这种丰富的反馈生态使其能够持续自适应、自我改进和进行长期优化。\n",
    "\n",
    "**4.多模态感知** 为了在真实环境中运行并响应复杂指令，AI 智能体具备**多模态感知**能力——不仅处理文本，还能处理视觉、听觉，甚至触觉或环境传感器数据。多模态大模型（MLLMs）的发展正是这种飞跃的典型，它使智能体能够跨越多种模态进行理解与推理，极大扩展了它们的智能与适用范围。\n",
    "\n",
    "**5.高级能力**  在环境、指导、反馈和感知等维度上的进步，共同推动了智能体内部能力的演化。动态环境、更丰富的指令与反馈以及增强的感知能力，让 AI 智能体远超普通对话机器人。它们现在展现出复杂规划、持久记忆、自适应推理和自主执行任务的能力。这标志着智能系统的一次变革性飞跃，展示了外部需求与内部突破如何共同驱动从被动应答的 LLM 聊天机器人向自主 AI 智能体的转变。\n",
    "\n",
    "### 总结\n",
    "换句话说，当AI只是LLM聊天机器人时，我们人类和AI对话，便可以评估它的水平，聊天能力。但现在Agent的环境复杂，能力增强，使得聊天评估不够了。因而，我们需要全方面对Agent新涌现出的各类能力进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28001a37",
   "metadata": {},
   "source": [
    "\n",
    "## 4.4 智能体性能评估方法\n",
    "\n",
    "\n",
    "| 评估方式      | 优点                         |     缺点                                    | \n",
    "|--------------|-----------------------------|---------------------------------------------|\n",
    "| 人工肉眼评估   | 方便，直观                    |   随机性强                                  |\n",
    "| 自动化评估指标  |  规模化，客观                |    部署难，需要大规模人工打标签，设计指标     |\n",
    "| LLM评估       |  不用人工打标签，可以有主观分析  |      运行慢，不一定准确                       |\n",
    "\n",
    "\n",
    "### 4.4.1 人工测试与用户反馈\n",
    "对于初学者来说，人工测试和收集用户反馈是一种非常直观且有效的评估方法。我们可以通过手工尝试各种任务，观察智能体的表现，从而对其性能有一个初步的了解。例如，我们可以给智能体提出一些问题，看看它是否能够准确地回答；或者让它完成一些简单的任务，观察它是否能够顺利完成。在这个过程中，我们可以记录下智能体的表现，包括它回答的准确性、任务完成的速度等方面。\n",
    "同时，用户反馈也是评估智能体性能的重要依据。用户在使用智能体的过程中，可能会遇到各种问题，他们的反馈可以帮助我们发现智能体的不足之处。例如，用户可能会告诉我们智能体的回答不够准确，或者在执行任务时出现了错误。通过收集和分析用户反馈，我们可以对智能体进行针对性的改进，提高其性能和用户体验。\n",
    "\n",
    "我们以代码任务为例，最直观的评测智能体的方式便是，直接用肉眼看它输出的代码正不正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952f48cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i+1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) <= threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(has_close_elements([1.0, 2.0, 3.0], 0.5))\n",
      "    print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device=device  # 指定设备\n",
    ")\n",
    "\n",
    "# 输入提示文本，这里的任务是检查列表中是否有两个数字的差小于给定的阈值，按照要求生成代码\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "from typing import List\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    # Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
    "    # >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    # False\n",
    "    # >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    # True\n",
    "    # Your Code Here\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "# print(prompt)\n",
    "# 生成续写文本\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=2000,  # 最大生成长度\n",
    "    min_length=50,   # 最小生成长度\n",
    "    do_sample=True,  # 是否使用采样\n",
    "    early_stopping=True  # 提前停止生成\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617f116",
   "metadata": {},
   "source": [
    "根据这个结果，我们可以看到，对于这个简单的任务，智能体生成了一段代码，用for循环来解答了这一编程问题。直观上来说，用户可以直接通过眼睛来检查其生成，这道题生成的代码是对的，从而完成对智能体最简单的评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90b8ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4.4.2 自动评估指标\n",
    "除了人工测试和用户反馈，我们还可以使用一些自动评估指标来评估智能体的性能。这些指标可以帮助我们更客观、更准确地评估智能体的表现。例如，任务完成率是一个常用的评估指标，它表示智能体成功完成任务的比例。通过计算任务完成率，我们可以了解智能体在不同任务中的表现，从而对其性能进行评估。\n",
    "答案准确率也是评估智能体性能的重要指标之一。它表示智能体给出正确答案的比例。通过计算答案准确率，我们可以了解智能体在回答问题时的准确性，从而对其性能进行评估。此外，我们还可以针对特定任务定义成功标准。例如，在解谜游戏中，我们可以将成功通关作为成功标准。通过将智能体的表现与成功标准进行对比，我们可以评估智能体在特定任务中的性能。\n",
    "\n",
    "### 4.4.3 基准数据集测试\n",
    "将自动评估指标聚集起来，公开到社区，这将会形成一种共识，从而成长为**基准数据集**。基准数据集测试是一种常用的评估方法，它帮助了不同人能在同一基准下对比模型，公开而公平。学术和开源社区提供了许多基准数据集，如问答基准、推理题库等。这些基准数据集包含了各种已知问题和相应的答案，我们可以用它们来测试智能体的水平。通过将智能体的表现与基准答案进行对比，我们可以评估智能体在不同任务中的性能。\n",
    "例如，我们可以使用问答基准数据集来测试智能体在回答问题时的表现。将智能体给出的答案与基准答案进行对比，计算答案的准确率，从而评估智能体在问答任务中的性能。这种方法可以帮助我们更客观地评估智能体的性能，发现其在不同任务中的优势和不足。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f87606",
   "metadata": {},
   "source": [
    "这里我们部署三个基准数据集，分别为 HumanEval，MMLU， GSM8K。其中HumanEval为代码生成任务的基准，MMLU（Massive Multitask Language Understanding）为多领域多任务问答的基准，GSM8K（Grade School Math）是数学推理任务的基准。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da600e9b",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bf6e5",
   "metadata": {},
   "source": [
    "先定义一个函数用于模型输入与输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7419b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假设你有一个函数 generate_one_completion 用于生成模型输出\n",
    "def generate_one_completion(prompt):\n",
    "    # 在这里调用你的模型生成代码\n",
    "    generated_text = text_generator(\n",
    "        prompt,\n",
    "        max_length=2000,  # 最大生成长度\n",
    "        min_length=50,   # 最小生成长度\n",
    "        return_full_text=False, # 重要！ 是否返回完整文本\n",
    "        do_sample=True,  # 是否使用采样\n",
    "        early_stopping=True  # 提前停止生成\n",
    "    )[0]['generated_text']\n",
    "    \n",
    "    return generated_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa27e3",
   "metadata": {},
   "source": [
    "随后，把humaneval中的数据问题加载进来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259df05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading problems...\n",
      "Loaded 164 problems.\n"
     ]
    }
   ],
   "source": [
    "from human_eval.data import write_jsonl, read_problems\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "print(\"Loading problems...\")\n",
    "problems = read_problems()\n",
    "print(f\"Loaded {len(problems)} problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ce19f",
   "metadata": {},
   "source": [
    "接下来，遍历每个问题，调用智能体来给出每个问题的答案（即写出每个问题的代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad45b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 9/164 [00:38<06:01,  2.33s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 164/164 [10:19<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_samples_per_task = 1 # 每个任务生成的样本数量\n",
    "samples = []\n",
    "# 生成样本\n",
    "keys = list(problems.keys())\n",
    "num = len(list(problems.keys()))\n",
    "for i in tqdm(range(num)):\n",
    "    completion = generate_one_completion(problems[keys[i]][\"prompt\"])\n",
    "    samples.append(dict(task_id=keys[i], completion=completion))\n",
    "# for _ in range(num_samples_per_task):\n",
    "#     # 遍历所有问题\n",
    "#     # tqdm 是一个进度条库，可以显示循环的进度\n",
    "#     # 这里我们遍历所有问题，并为每个问题生成一个样本 \n",
    "#     for task_id in tqdm(problems):\n",
    "#         # print(f\"Generating completion for task: {task_id}\")\n",
    "#         # 生成模型输出\n",
    "#         completion = generate_one_completion(problems[task_id][\"prompt\"])\n",
    "#         # 将生成的样本添加到列表中\n",
    "#         samples.append(dict(task_id=task_id, completion=completion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11306303",
   "metadata": {},
   "source": [
    "\n",
    "将生成的样本写入 JSONL 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a594f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing samples to JSONL file...\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing samples to JSONL file...\")\n",
    "write_jsonl(\"samples.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8455d",
   "metadata": {},
   "source": [
    "查看刚才生成的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9fa76b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"task_id\": \"HumanEval/0\", \"completion\": \"    # TODO: implement this function\\n\\n    for i in range(len(numbers)):\\n        for j in range(i+1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) <= threshold:\\n                return True\\n    return False\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    print(has_close_elements([1.0, 2.0, 3.0], 0.5))\\n    print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))\"}\n",
      "{\"task_id\": \"HumanEval/1\", \"completion\": \"    result = []\\n    i = 0\\n    while i < len(paren_string):\\n        if paren_string[i] == '(':\\n            i += 1\\n        elif paren_string[i] == ')':\\n            j = i + 1\\n            while paren_string[j]!= ')' and j <= len(paren_string)-1:\\n                j += 1\\n            result.append(paren_string[i:j])\\n            i = j\\n        else:\\n            i += 1\\n\\n    return result\\n\\n\\nif __name__ == '__main__':\\n    print(separate_paren_groups(\\\"( ) (( )) (( )( ))\\\"))\"}\n",
      "{\"task_id\": \"HumanEval/2\", \"completion\": \"    return number - int(number)\\n\\n# Test cases\\nprint(truncate_number(3.5))  # Expected output: 0.5\\nprint(truncate_number(4.6))  # Expected output: 0.6\\nprint(truncate_number(-2.8)) # Expected output: -0.8\\n```\\n\\nThe provided code snippet defines a function `truncate_number` that takes a single parameter `number`, which is a positive floating-point number. The function calculates and returns the decimal part of this number by subtracting its integer part from the original number.\\n\\nTo verify the correctness of the function, several test cases are included in the docstring. Each test case provides a specific value for `number` and an expected result for the decimal part obtained using the `truncate_number` function. By running these tests, you can confirm whether the implementation meets the requirements or if there are any edge cases that need further refinement.\\n\\nIn summary, the solution involves understanding floating-point arithmetic, specifically focusing on how to handle and extract"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the fractional part of a number. This problem requires knowledge of Python's built-in functions like `int()` and `float()`, as well as familiarity with basic arithmetic operations. The provided code demonstrates how to implement such functionality in a concise manner while adhering to the guidelines provided. ```python\\nimport math\\n\\ndef calculate_truncation(a):\\n    \\\"\\\"\\\"\\n    Calculate the truncation of a number, i.e., the part of the number after the decimal point.\\n    \\n    Parameters:\\n    a (float): A positive floating-point number\\n    \\n    Returns:\\n    float: The truncated version of the input number.\\n    \\\"\\\"\\\"\\n    # Using math.trunc to get the integer part and then subtracting it from the original number\\n    trunc = math.floor(abs(a))\\n    # Subtracting the integer part from the absolute value of the number to get the fractional part\\n    frac = abs(a) - trunc\\n    return trunc, frac\\n\\n# Function to check the correctness of the calculate_truncation function\\ndef check_function():\\n    assert calculate_truncation(3.5) == (3.0, 0.5), \\\"Test case 1 failed\\\"\\n    assert calculate_truncation(4.6) == (4.0, 0.6), \\\"Test case 2 failed\\\"\\n    assert calculate_truncation(-2.8) == (-3.0, 0.2), \\\"Test case 3 failed\\\"\\n    print(\\\"All test cases passed!\\\")\\n\\ncheck_function()\\n```\"}\n",
      "{\"task_id\": \"HumanEval/3\", \"completion\": \"    acc = 0\\n\\n    for op in operations:\\n        acc += op\\n        if acc < 0:\\n            return True\\n    return False\\n\\n\\nif __name__ == '__main__':\\n    print(below_zero([1, 2, 3]))  # Expected output: False\\n    print(below_zero([1, 2, -4, 5]))  # Expected output: True\\n    print(below_zero([]))  # Expected output: False\\n    print(below_zero([-2, 1, -3, 4]))  # Expected output: True\\n    print(below_zero([5, -7, 8, -9, 10, 11, 12]))  # Expected output: True\\n    print(below_zero([5, 6, 7, 8, 9, 10, 11, 12, 13]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]))  # Expected output: True\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]))  # Expected output: False\\n    print(below_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, \"}\n",
      "{\"task_id\": \"HumanEval/4\", \"completion\": \"    total = sum(abs(n - m) for n, m in zip(numbers, [np.mean(numbers), ]))\\n    return total / len(numbers)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    print(mean_absolute_deviation([1.0, 2.0, 3.0, 4.0]))\"}\n",
      "{\"task_id\": \"HumanEval/5\", \"completion\": \"    return numbers + [delimeter] * (len(numbers) - 1)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    import doctest\\n\\n    print(doctest.testmod())\"}\n",
      "{\"task_id\": \"HumanEval/6\", \"completion\": \"    result = []\\n    paren_stack = []\\n    i = 0\\n    while i < len(paren_string):\\n        if paren_string[i] == '(':\\n            paren_stack.append(i)\\n        elif paren_string[i] == ')':\\n            start_index = paren_stack.pop()\\n            end_index = i\\n            depth = (end_index - start_index) + 1\\n            result.append(depth)\\n            i = end_index\\n        else:\\n            i += 1\\n\\n    return result\\n\\n\\nif __name__ == '__main__':\\n    print(parse_nested_parens('(()()) ((())) () ((())()())'))\"}\n",
      "{\"task_id\": \"HumanEval/7\", \"completion\": \"    return [s for s in strings if substring in s]\"}\n",
      "{\"task_id\": \"HumanEval/8\", \"completion\": \"    if not numbers:\\n        return (0, 1)\\n\\n    total_sum = 0\\n    for number in numbers:\\n        total_sum += number\\n\\n    total_product = 1\\n    for number in numbers:\\n        total_product *= number\\n\\n    return (total_sum, total_product)\\n\\n\\nif __name__ == '__main__':\\n    import doctest\\n\\n    doctest.testmod()\"}\n",
      "{\"task_id\": \"HumanEval/9\", \"completion\": \"    max_elements = []\\n    for i in range(len(numbers)):\\n        if len(max_elements) == 0:\\n            max_elements.append(numbers[i])\\n        else:\\n            max_elements.append(max(max_elements[-1], numbers[i]))\\n    return max_elements\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    from doctest import testmod\\n\\n    print(rolling_max([1, 2, 3, 2, 3, 4, 2]))  # (1, 2, 3, 3, 3, 4, 4)\\n    print(rolling_max([5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]))  # (5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\\n    print(rolling_max([]))  # []\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"head samples.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505fdff",
   "metadata": {},
   "source": [
    "调用humaneval提供的脚本来评估生成的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36e9f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 11735.52it/s]\n",
      " 98%|█████████▊| 160/164 [00:01<00:00, 135.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to samples.jsonl_results.jsonl...\n",
      "{'pass@1': 0.1524390243902439}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:04<00:00, 37.94it/s] \n",
      "100%|██████████| 164/164 [00:00<00:00, 60461.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"evaluate_functional_correctness samples.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b873e9",
   "metadata": {},
   "source": [
    "可以看到，pass@1的正确率是15%，也就是说，每道题只生成一次代码，正确的概率是15%。而如果多生成几次，理论上效果是能提高的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113755f",
   "metadata": {},
   "source": [
    "### MMLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51d4f5",
   "metadata": {},
   "source": [
    "大家可以发现，我们如果对每个benchmark都单独部署一次环境，会非常的麻烦。因为每个benchmark的代码都不一样，需要的环境库也不一样。因而，有没有一个集成式的仓库，可以把主流的benchmark都封装好，集成进来呢？\n",
    "\n",
    "有的，这里推荐大家harness库，它把主流benchmark都集成进来，不需要重复配环境，只需要配一次，一键测评智能体。但这样的坏处也是有的，封装的太好的话，可以自主调整的地方就少（例如prompt，例如RAG等等）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b705e6",
   "metadata": {},
   "source": [
    "### 配置Harness环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bash命令，将evaluation-harness克隆到当前目录并安装\n",
    "\n",
    "git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db825299",
   "metadata": {},
   "source": [
    "### 一键测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b26b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03:22:55:58 INFO     [__main__:440] Selected Tasks: ['mmlu']\n",
      "2025-07-03:22:55:58 WARNING  [evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-07-03:22:55:58 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-07-03:22:55:58 INFO     [evaluator:227] Initializing hf model, with arguments: {'pretrained': '../models/Qwen2.5-0.5B-Instruct/'}\n",
      "2025-07-03:22:55:58 INFO     [models.huggingface:138] Using device 'cuda:0'\n",
      "2025-07-03:22:55:59 INFO     [models.huggingface:391] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "2025-07-03:22:58:49 INFO     [api.task:434] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:00<00:00, 821.04it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:00<00:00, 826.07it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 823.75it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:00<00:00, 827.58it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 771.07it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 799.87it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:00<00:00, 839.31it/s]\n",
      "2025-07-03:22:58:50 INFO     [api.task:434] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 751.30it/s]\n",
      "2025-07-03:22:58:51 INFO     [api.task:434] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 837.10it/s]\n",
      "2025-07-03:22:58:51 INFO     [api.task:434] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:00<00:00, 805.97it/s]\n",
      "2025-07-03:22:58:51 INFO     [api.task:434] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:00<00:00, 391.65it/s]\n",
      "2025-07-03:22:58:51 INFO     [api.task:434] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:00<00:00, 830.56it/s]\n",
      "2025-07-03:22:58:51 INFO     [api.task:434] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:00<00:00, 835.98it/s]\n",
      "2025-07-03:22:58:52 INFO     [api.task:434] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:00<00:00, 836.64it/s]\n",
      "2025-07-03:22:58:52 INFO     [api.task:434] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:00<00:00, 843.53it/s]\n",
      "2025-07-03:22:58:52 INFO     [api.task:434] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:00<00:00, 803.64it/s]\n",
      "2025-07-03:22:58:52 INFO     [api.task:434] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 781.61it/s]\n",
      "2025-07-03:22:58:53 INFO     [api.task:434] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:00<00:00, 836.81it/s]\n",
      "2025-07-03:22:58:53 INFO     [api.task:434] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:00<00:00, 836.50it/s]\n",
      "2025-07-03:22:58:53 INFO     [api.task:434] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:00<00:00, 809.00it/s]\n",
      "2025-07-03:22:58:54 INFO     [api.task:434] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 840.55it/s]\n",
      "2025-07-03:22:58:54 INFO     [api.task:434] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:00<00:00, 846.36it/s]\n",
      "2025-07-03:22:58:54 INFO     [api.task:434] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:00<00:00, 837.51it/s]\n",
      "2025-07-03:22:58:55 INFO     [api.task:434] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:00<00:00, 835.23it/s]\n",
      "2025-07-03:22:58:55 INFO     [api.task:434] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:00<00:00, 834.36it/s]\n",
      "2025-07-03:22:58:55 INFO     [api.task:434] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:00<00:00, 848.65it/s]\n",
      "2025-07-03:22:58:56 INFO     [api.task:434] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 843.33it/s]\n",
      "2025-07-03:22:58:56 INFO     [api.task:434] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:00<00:00, 811.52it/s]\n",
      "2025-07-03:22:58:56 INFO     [api.task:434] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:00<00:00, 855.10it/s]\n",
      "2025-07-03:22:58:56 INFO     [api.task:434] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:00<00:00, 851.78it/s]\n",
      "2025-07-03:22:58:57 INFO     [api.task:434] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:00<00:00, 850.69it/s]\n",
      "2025-07-03:22:58:57 INFO     [api.task:434] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 857.78it/s]\n",
      "2025-07-03:22:58:57 INFO     [api.task:434] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:00<00:00, 854.10it/s]\n",
      "2025-07-03:22:58:57 INFO     [api.task:434] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:00<00:00, 855.11it/s]\n",
      "2025-07-03:22:58:57 INFO     [api.task:434] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:00<00:00, 846.05it/s]\n",
      "2025-07-03:22:58:58 INFO     [api.task:434] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:00<00:00, 856.28it/s]\n",
      "2025-07-03:22:58:58 INFO     [api.task:434] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:00<00:00, 857.50it/s]\n",
      "2025-07-03:22:58:59 INFO     [api.task:434] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:00<00:00, 770.55it/s]\n",
      "2025-07-03:22:58:59 INFO     [api.task:434] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:00<00:00, 838.61it/s]\n",
      "2025-07-03:22:59:00 INFO     [api.task:434] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:00<00:00, 849.48it/s]\n",
      "2025-07-03:22:59:00 INFO     [api.task:434] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:00<00:00, 819.38it/s]\n",
      "2025-07-03:22:59:00 INFO     [api.task:434] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:00<00:00, 859.86it/s]\n",
      "2025-07-03:22:59:00 INFO     [api.task:434] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:00<00:00, 791.17it/s]\n",
      "2025-07-03:22:59:01 INFO     [api.task:434] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:00<00:00, 852.03it/s]\n",
      "2025-07-03:22:59:01 INFO     [api.task:434] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:00<00:00, 853.82it/s]\n",
      "2025-07-03:22:59:01 INFO     [api.task:434] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:00<00:00, 856.27it/s]\n",
      "2025-07-03:22:59:01 INFO     [api.task:434] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:02<00:00, 755.16it/s]\n",
      "2025-07-03:22:59:03 INFO     [api.task:434] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:01<00:00, 838.27it/s]\n",
      "2025-07-03:22:59:05 INFO     [api.task:434] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:00<00:00, 797.32it/s]\n",
      "2025-07-03:22:59:05 INFO     [api.task:434] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:00<00:00, 864.32it/s]\n",
      "2025-07-03:22:59:05 INFO     [api.task:434] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:00<00:00, 812.01it/s]\n",
      "2025-07-03:22:59:05 INFO     [api.task:434] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:00<00:00, 855.12it/s]\n",
      "2025-07-03:22:59:06 INFO     [api.task:434] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:00<00:00, 856.47it/s]\n",
      "2025-07-03:22:59:06 INFO     [api.task:434] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:00<00:00, 857.24it/s]\n",
      "2025-07-03:22:59:06 INFO     [api.task:434] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:00<00:00, 854.53it/s]\n",
      "2025-07-03:22:59:06 INFO     [api.task:434] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:00<00:00, 861.43it/s]\n",
      "2025-07-03:22:59:07 INFO     [api.task:434] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:00<00:00, 858.53it/s]\n",
      "2025-07-03:22:59:07 INFO     [evaluator:559] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 56168/56168 [01:00<00:00, 934.97it/s] \n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2025-07-03:23:00:35 INFO     [loggers.evaluation_tracker:280] Output path not provided, skipping saving results aggregated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=../models/Qwen2.5-0.5B-Instruct/), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |↑  |0.4577|±  |0.0041|\n",
      "| - humanities                          |      2|none  |      |acc   |↑  |0.4227|±  |0.0069|\n",
      "|  - formal_logic                       |      1|none  |     0|acc   |↑  |0.2937|±  |0.0407|\n",
      "|  - high_school_european_history       |      1|none  |     0|acc   |↑  |0.6000|±  |0.0383|\n",
      "|  - high_school_us_history             |      1|none  |     0|acc   |↑  |0.5441|±  |0.0350|\n",
      "|  - high_school_world_history          |      1|none  |     0|acc   |↑  |0.6160|±  |0.0317|\n",
      "|  - international_law                  |      1|none  |     0|acc   |↑  |0.7355|±  |0.0403|\n",
      "|  - jurisprudence                      |      1|none  |     0|acc   |↑  |0.5833|±  |0.0477|\n",
      "|  - logical_fallacies                  |      1|none  |     0|acc   |↑  |0.4601|±  |0.0392|\n",
      "|  - moral_disputes                     |      1|none  |     0|acc   |↑  |0.5376|±  |0.0268|\n",
      "|  - moral_scenarios                    |      1|none  |     0|acc   |↑  |0.2380|±  |0.0142|\n",
      "|  - philosophy                         |      1|none  |     0|acc   |↑  |0.4855|±  |0.0284|\n",
      "|  - prehistory                         |      1|none  |     0|acc   |↑  |0.5340|±  |0.0278|\n",
      "|  - professional_law                   |      1|none  |     0|acc   |↑  |0.3553|±  |0.0122|\n",
      "|  - world_religions                    |      1|none  |     0|acc   |↑  |0.5906|±  |0.0377|\n",
      "| - other                               |      2|none  |      |acc   |↑  |0.5076|±  |0.0088|\n",
      "|  - business_ethics                    |      1|none  |     0|acc   |↑  |0.5300|±  |0.0502|\n",
      "|  - clinical_knowledge                 |      1|none  |     0|acc   |↑  |0.5019|±  |0.0308|\n",
      "|  - college_medicine                   |      1|none  |     0|acc   |↑  |0.4393|±  |0.0378|\n",
      "|  - global_facts                       |      1|none  |     0|acc   |↑  |0.2900|±  |0.0456|\n",
      "|  - human_aging                        |      1|none  |     0|acc   |↑  |0.5426|±  |0.0334|\n",
      "|  - management                         |      1|none  |     0|acc   |↑  |0.5728|±  |0.0490|\n",
      "|  - marketing                          |      1|none  |     0|acc   |↑  |0.7479|±  |0.0284|\n",
      "|  - medical_genetics                   |      1|none  |     0|acc   |↑  |0.4700|±  |0.0502|\n",
      "|  - miscellaneous                      |      1|none  |     0|acc   |↑  |0.5594|±  |0.0178|\n",
      "|  - nutrition                          |      1|none  |     0|acc   |↑  |0.5850|±  |0.0282|\n",
      "|  - professional_accounting            |      1|none  |     0|acc   |↑  |0.3262|±  |0.0280|\n",
      "|  - professional_medicine              |      1|none  |     0|acc   |↑  |0.3787|±  |0.0295|\n",
      "|  - virology                           |      1|none  |     0|acc   |↑  |0.4337|±  |0.0386|\n",
      "| - social sciences                     |      2|none  |      |acc   |↑  |0.5310|±  |0.0089|\n",
      "|  - econometrics                       |      1|none  |     0|acc   |↑  |0.3158|±  |0.0437|\n",
      "|  - high_school_geography              |      1|none  |     0|acc   |↑  |0.5455|±  |0.0355|\n",
      "|  - high_school_government_and_politics|      1|none  |     0|acc   |↑  |0.5440|±  |0.0359|\n",
      "|  - high_school_macroeconomics         |      1|none  |     0|acc   |↑  |0.4410|±  |0.0252|\n",
      "|  - high_school_microeconomics         |      1|none  |     0|acc   |↑  |0.4790|±  |0.0324|\n",
      "|  - high_school_psychology             |      1|none  |     0|acc   |↑  |0.6257|±  |0.0207|\n",
      "|  - human_sexuality                    |      1|none  |     0|acc   |↑  |0.5420|±  |0.0437|\n",
      "|  - professional_psychology            |      1|none  |     0|acc   |↑  |0.4657|±  |0.0202|\n",
      "|  - public_relations                   |      1|none  |     0|acc   |↑  |0.5455|±  |0.0477|\n",
      "|  - security_studies                   |      1|none  |     0|acc   |↑  |0.5551|±  |0.0318|\n",
      "|  - sociology                          |      1|none  |     0|acc   |↑  |0.6617|±  |0.0335|\n",
      "|  - us_foreign_policy                  |      1|none  |     0|acc   |↑  |0.7300|±  |0.0446|\n",
      "| - stem                                |      2|none  |      |acc   |↑  |0.3892|±  |0.0085|\n",
      "|  - abstract_algebra                   |      1|none  |     0|acc   |↑  |0.3300|±  |0.0473|\n",
      "|  - anatomy                            |      1|none  |     0|acc   |↑  |0.4148|±  |0.0426|\n",
      "|  - astronomy                          |      1|none  |     0|acc   |↑  |0.4605|±  |0.0406|\n",
      "|  - college_biology                    |      1|none  |     0|acc   |↑  |0.4583|±  |0.0417|\n",
      "|  - college_chemistry                  |      1|none  |     0|acc   |↑  |0.2800|±  |0.0451|\n",
      "|  - college_computer_science           |      1|none  |     0|acc   |↑  |0.3500|±  |0.0479|\n",
      "|  - college_mathematics                |      1|none  |     0|acc   |↑  |0.2900|±  |0.0456|\n",
      "|  - college_physics                    |      1|none  |     0|acc   |↑  |0.3137|±  |0.0462|\n",
      "|  - computer_security                  |      1|none  |     0|acc   |↑  |0.6800|±  |0.0469|\n",
      "|  - conceptual_physics                 |      1|none  |     0|acc   |↑  |0.3787|±  |0.0317|\n",
      "|  - electrical_engineering             |      1|none  |     0|acc   |↑  |0.4966|±  |0.0417|\n",
      "|  - elementary_mathematics             |      1|none  |     0|acc   |↑  |0.3360|±  |0.0243|\n",
      "|  - high_school_biology                |      1|none  |     0|acc   |↑  |0.5387|±  |0.0284|\n",
      "|  - high_school_chemistry              |      1|none  |     0|acc   |↑  |0.3990|±  |0.0345|\n",
      "|  - high_school_computer_science       |      1|none  |     0|acc   |↑  |0.4500|±  |0.0500|\n",
      "|  - high_school_mathematics            |      1|none  |     0|acc   |↑  |0.2963|±  |0.0278|\n",
      "|  - high_school_physics                |      1|none  |     0|acc   |↑  |0.2450|±  |0.0351|\n",
      "|  - high_school_statistics             |      1|none  |     0|acc   |↑  |0.3148|±  |0.0317|\n",
      "|  - machine_learning                   |      1|none  |     0|acc   |↑  |0.3929|±  |0.0464|\n",
      "\n",
      "|      Groups      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu              |      2|none  |      |acc   |↑  |0.4577|±  |0.0041|\n",
      "| - humanities     |      2|none  |      |acc   |↑  |0.4227|±  |0.0069|\n",
      "| - other          |      2|none  |      |acc   |↑  |0.5076|±  |0.0088|\n",
      "| - social sciences|      2|none  |      |acc   |↑  |0.5310|±  |0.0089|\n",
      "| - stem           |      2|none  |      |acc   |↑  |0.3892|±  |0.0085|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# 指定模型路径\n",
    "lm_eval_path = \"lm_eval\"\n",
    "model_path = \"../models/Qwen2.5-0.5B-Instruct/\"\n",
    "# 切换到 lm-evaluation-harness 目录\n",
    "os.system(\"cd lm-evaluation-harness\")\n",
    "# 评估模型在 MMLU 上的性能\n",
    "os.system(f\"lm_eval --model hf     --model_args pretrained={model_path}     --tasks mmlu     --device cuda:0     --batch_size 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a979ab4",
   "metadata": {},
   "source": [
    "从这个实验结果可以看到，mmlu包含了多种多样的任务，包括历史文学，医学药学，等等。Qwen-0.5B已经在上面取得了不错的结果，问题的准确率高达50%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2592c",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5726ca5",
   "metadata": {},
   "source": [
    "通过harness仓库，我们可以继续测评GSM8K这个基准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c39151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03:23:48:15 INFO     [__main__:440] Selected Tasks: ['gsm8k']\n",
      "2025-07-03:23:48:15 WARNING  [evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-07-03:23:48:15 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-07-03:23:48:15 INFO     [evaluator:227] Initializing hf model, with arguments: {'pretrained': '../models/Qwen2.5-0.5B-Instruct/'}\n",
      "2025-07-03:23:48:15 INFO     [models.huggingface:138] Using device 'cuda:0'\n",
      "2025-07-03:23:48:15 INFO     [models.huggingface:391] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 549692.81 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 509231.13 examples/s]\n",
      "2025-07-03:23:48:30 INFO     [evaluator:290] gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}\n",
      "2025-07-03:23:48:30 INFO     [api.task:434] Building contexts for gsm8k on rank 0...\n",
      "100%|██████████| 1319/1319 [00:03<00:00, 339.66it/s]\n",
      "2025-07-03:23:48:33 INFO     [evaluator:559] Running generate_until requests\n",
      "Running generate_until requests:   0%|          | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Running generate_until requests: 100%|██████████| 1319/1319 [16:24<00:00,  1.34it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2025-07-04:00:05:01 INFO     [loggers.evaluation_tracker:280] Output path not provided, skipping saving results aggregated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=../models/Qwen2.5-0.5B-Instruct/), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.3177|±  |0.0128|\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.2138|±  |0.0113|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "lm_eval_path = \"lm_eval\"\n",
    "model_path = \"../models/Qwen2.5-0.5B-Instruct/\"\n",
    "os.system(\"cd lm-evaluation-harness\")\n",
    "# 评估模型在 GSM8K 上的性能\n",
    "os.system(f\"lm_eval --model hf     --model_args pretrained={model_path}     --tasks gsm8k     --device cuda:0     --batch_size 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaba6b7",
   "metadata": {},
   "source": [
    "可以看到，数学问题还是比较难的，Qwen-0.5B在即便是小学级别难度的GSM8K，依旧只有30%左右的正确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5c8fb",
   "metadata": {},
   "source": [
    "### HumanEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa290d65",
   "metadata": {},
   "source": [
    "同样的，我们可以发现，刚才手动部署的HumanEval也被集成在Harness环境中，便于部署。但刚才讲的问题随之而来，那就是能改动的地方很少。相比于刚才手动部署，我们能轻易地改动prompt，加东西，集成后的测试脚本冗长复杂，比较难对其进行改动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43509a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04:11:34:04 INFO     [__main__:440] Selected Tasks: ['humaneval']\n",
      "2025-07-04:11:34:04 WARNING  [evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-07-04:11:34:04 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-07-04:11:34:04 INFO     [evaluator:227] Initializing hf model, with arguments: {'pretrained': '../models/Qwen2.5-0.5B-Instruct/'}\n",
      "2025-07-04:11:34:04 INFO     [models.huggingface:138] Using device 'cuda:0'\n",
      "2025-07-04:11:34:04 INFO     [models.huggingface:391] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "Downloading builder script: 9.18kB [00:00, 30.0MB/s]\n",
      "Downloading extra modules: 6.10kB [00:00, 27.0MB/s]\n",
      "2025-07-04:11:34:15 INFO     [evaluator:290] humaneval: Using gen_kwargs: {'until': ['\\nclass', '\\ndef', '\\n#', '\\nif', '\\nprint'], 'max_gen_toks': 1024, 'do_sample': False}\n",
      "2025-07-04:11:34:15 INFO     [api.task:434] Building contexts for humaneval on rank 0...\n",
      "100%|██████████| 164/164 [00:00<00:00, 3689.18it/s]\n",
      "2025-07-04:11:34:15 INFO     [evaluator:559] Running generate_until requests\n",
      "Running generate_until requests:   0%|          | 0/164 [00:00<?, ?it/s]/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Running generate_until requests: 100%|██████████| 164/164 [05:40<00:00,  2.07s/it]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2025-07-04:11:40:04 INFO     [loggers.evaluation_tracker:280] Output path not provided, skipping saving results aggregated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=../models/Qwen2.5-0.5B-Instruct/), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\n",
      "|  Tasks  |Version|  Filter   |n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------|------:|-----------|-----:|------|---|-----:|---|-----:|\n",
      "|humaneval|      1|create_test|     0|pass@1|   |0.2683|±  |0.0347|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "lm_eval_path = \"lm_eval\"\n",
    "model_path = \"../models/Qwen2.5-0.5B-Instruct/\"\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.system(\"cd lm-evaluation-harness\")\n",
    "# 评估模型在 HumanEval 上的性能\n",
    "# 注意：如果你使用的是 Qwen2.5-0.5B-Instruct 模型，可能需要设置环境变量 HF_ALLOW_CODE_EVAL=1 来允许代码评估\n",
    "# 另外，使用 --confirm_run_unsafe_code 参数来确认运行不安全的代码\n",
    "# 这可能会导致安全风险，请确保你了解风险并在安全的环境中\n",
    "os.system(f\"lm_eval --model hf     --model_args pretrained={model_path}     --tasks humaneval     --device cuda:0     --batch_size 8 --confirm_run_unsafe_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33c8357",
   "metadata": {},
   "source": [
    "不过效果还是不错的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1073a81",
   "metadata": {},
   "source": [
    "### 4.4.4 LLM自动评测\n",
    "相比于人工肉眼评测，以及固定基准数据集评测，一种新兴的评测方法已经涌现。LLM-as-a-Judge（大语言模型作为裁判）是一种利用大语言模型自动评估其他模型输出质量的方法。通过使用经过训练的LLM作为评判者，它可以对生成任务（如问答、文本生成或代码生成）的输出进行评分、分类和质量评估，提供一致且高效的自动评测。相比传统的人工评测，LLM-as-a-Judge大幅提升了评测速度和效率，并能够实时提供反馈，支持模型的快速迭代。尽管如此，LLM的评测标准和效果仍然依赖于其训练数据和任务复杂度，可能在某些情况下存在偏差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f3ff7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里通过prompt设计几个指标，然后调用Qwen评价自己\n",
    "llm_as_a_judge_prompt_pre = \\\n",
    "\"\"\" \n",
    "Prompt:\n",
    "\n",
    "You are a code review expert responsible for evaluating the quality of the following code. Please assess the generated code based on the following criteria and provide a score from 1 to 10, where 1 means the code quality is very poor, and 10 means the code quality is excellent.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "Correctness: Does the code correctly solve the problem and is it free of syntax or runtime errors?\n",
    "\n",
    "Conciseness: Is the code concise and efficient, avoiding unnecessary implementations?\n",
    "\n",
    "Readability: Is the code clear and understandable, with descriptive variable names and sufficient comments?\n",
    "\n",
    "Performance: Does the code consider performance optimization and avoid potential performance bottlenecks?\n",
    "\n",
    "Scalability: Is the code easily extensible for future modifications or feature additions?\n",
    "\n",
    "Code:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm_as_a_judge_prompt_post = \\\n",
    "\"\"\"\n",
    "\n",
    "Scoring Guidelines:\n",
    "\n",
    "1-3: The code has significant errors, does not work properly, or is overly complex and difficult to understand.\n",
    "\n",
    "4-6: The code solves the problem but has some redundancy or areas for improvement in terms of performance, readability, or structure.\n",
    "\n",
    "7-9: The code is correct and efficient, well-structured, and easy to understand, with minor room for improvement.\n",
    "\n",
    "10: The code is perfect, adhering to best practices, concise, efficient, readable, and performs well.\n",
    "\n",
    "Evaluation Result:\n",
    "\n",
    "Score:\n",
    "\n",
    "Review Explanation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e4c9ee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "The code snippet provided implements a method called `has_close_elements`, which takes a list of floating-point numbers and a threshold value as input. It checks whether there exist any two distinct numbers in the list that are within the specified distance (`threshold`) from each other. This method leverages the `sorted()` function to first sort the list of numbers, then iteratively compares each element with every other element in the sorted list, ensuring that only one comparison is necessary to determine closeness.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. **Error Handling**: There's no error handling mechanism built into the code. If the input list contains non-float values or if the threshold is too high, an exception could be raised.\n",
      "  \n",
      "2. **Complexity**: The code is inefficient due to repeated comparisons and sorting operations. A more optimized version might involve calculating distances between elements without resorting to nested loops.\n",
      "\n",
      "3. **Readability**: While the code is self-explanatory, it lacks proper documentation and can be confusing for users unfamiliar with Python lists.\n",
      "\n",
      "4. **Performance**: Although the code is designed to run efficiently, it may perform poorly with large datasets, especially when dealing with extremely small thresholds.\n",
      "\n",
      "Given these points, I would rate the code at **6 out of 10** because while it works correctly for the examples provided, it suffers from inefficiency, complexity, and lack of thorough testing. To improve, it should include error handling, potentially use a different data structure (like a dictionary), and possibly optimize the calculation process.\n",
      "\n",
      "Please let me know if you need any clarification or additional insights! \n",
      "\n",
      "Review Explanation Continued:\n",
      "\n",
      "The code snippet provided implements a method called `has_close_elements`, which takes a list of floating-point numbers and a threshold value as input. It checks whether there exist any two distinct numbers in the list that are within the specified distance (`threshold`) from each other. This method leverages the `sorted()` function to first sort the list of numbers, then iteratively compares each element with every other element in the sorted list, ensuring that only one comparison is necessary to determine closeness.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. **Error Handling**: There's no error handling mechanism built into the code. If the input list contains non-float values or if the threshold is too high, an exception could be raised.\n",
      "   \n",
      "2. **Complexity**: The code is inefficient due to repeated comparisons and sorting operations. A more optimized version might involve calculating distances between elements without resorting to nested loops.\n",
      "\n",
      "3. **Readability**: While the code is self-explanatory, it lacks proper documentation and can be confusing for users unfamiliar with Python lists.\n",
      "\n",
      "4. **Performance**: Although the code is designed to run efficiently, it may perform poorly with large datasets, especially when dealing with extremely small thresholds.\n",
      "\n",
      "Given these points, I would rate the code at **6 out of 10** because while it works correctly for the examples provided, it suffers from inefficiency, complexity, and lack of thorough testing. To improve, it should include error handling, potentially use a different data structure (like a dictionary), and possibly optimize the calculation process.\n",
      "\n",
      "Please let me know if you need any clarification or additional insights! \n",
      "\n",
      "Review Explanation Continued:\n",
      "\n",
      "The code snippet provided implements a method called `has_close_elements`, which takes a list of floating-point numbers and a threshold value as input. It checks whether there exist any two distinct numbers in the list that are within the specified distance (`threshold`) from each other. This method leverages the `sorted()` function to first sort the list of numbers, then iteratively compares each element with every other element in the sorted list, ensuring that only one comparison is necessary to determine closeness.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. **Error Handling**: There's no error handling mechanism built into the code. If the input list contains non-float values or if the threshold is too high, an exception could be raised.\n",
      "   \n",
      "2. **Complexity**: The code is inefficient due to repeated comparisons and sorting operations. A more optimized version might involve calculating distances between elements without resorting to nested loops.\n",
      "\n",
      "3. **Readability**: While the code is self-explanatory, it lacks proper documentation and can be confusing for users unfamiliar with Python lists.\n",
      "\n",
      "4. **Performance**: Although the code is designed to run efficiently, it may perform poorly with large datasets, especially when dealing with extremely small thresholds.\n",
      "\n",
      "Given these points, I would rate the code at **6 out of 10** because while it works correctly for the examples provided, it suffers from inefficiency, complexity, and lack of thorough testing. To improve, it should include error handling, potentially use a different data structure (like a dictionary), and possibly optimize the calculation process.\n",
      "\n",
      "Please let me know if you need any clarification or additional insights! \n",
      "\n",
      "Review Explanation Continued:\n",
      "\n",
      "The code snippet provided implements a method called `has_close_elements`, which takes a list of floating-point numbers and a threshold value as input. It checks whether there exist any two distinct numbers in the list that are within the specified distance (`threshold`) from each other. This method leverages the `sorted()` function to first sort the list of numbers, then iteratively compares each element with every other element in the sorted list, ensuring that only one comparison is necessary to determine closeness.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. **Error Handling**: There's no error handling mechanism built into the code. If the input list contains non-float values or if the threshold is too high, an exception could be raised.\n",
      "   \n",
      "2. **Complexity**: The code is inefficient due to repeated comparisons and sorting operations. A more optimized version might involve calculating distances between elements without resorting to nested loops.\n",
      "\n",
      "3. **Readability**: While the code is self-explanatory, it lacks proper documentation and can be confusing for users unfamiliar with Python lists.\n",
      "\n",
      "4. **Performance**: Although the code is designed to run efficiently, it may perform poorly with large datasets, especially when dealing with extremely small thresholds.\n",
      "\n",
      "Given these points, I would rate the code at **6 out of 10** because while it works correctly for the examples provided, it suffers from inefficiency, complexity, and lack of thorough testing. To improve, it should include error handling, potentially use a different data structure (like a dictionary), and possibly optimize the calculation process.\n",
      "\n",
      "Please let me know if you need any clarification or additional insights! \n",
      "\n",
      "Review Explanation Continued:\n",
      "\n",
      "The code snippet provided implements a method called `has_close_elements`, which takes a list of floating-point numbers and a threshold value as input. It checks whether there exist any two distinct numbers in the list that are within the specified distance (`threshold`) from each other. This method leverages the `sorted()` function to first sort the list of numbers, then iteratively compares each element with every other element in the sorted list, ensuring that only one comparison is necessary to determine closeness.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. **Error Handling**: There's no error handling mechanism built into the code. If the input list contains non-float values or if the threshold is too high, an exception could be raised.\n",
      "   \n",
      "2. **Complexity**: The code is inefficient due to repeated comparisons and sorting operations. A\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device=device  # 指定设备\n",
    ")\n",
    "\n",
    "#\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "```python\n",
    "from typing import List\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    # Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
    "    # >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    # False\n",
    "    # >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    # True\n",
    "    for i in range(len(numbers)):\n",
    "        for j in range(i+1, len(numbers)):\n",
    "            if abs(numbers[i] - numbers[j]) < threshold:\n",
    "                return True\n",
    "    return False\n",
    "```\n",
    "This solution iterates through all possible pairs of numbers in the given list, calculates their absolute difference using the `abs` function, and checks if this difference is less than the provided threshold. If such a pair exists, it returns True; otherwise, it returns False after checking all pairs. The check function with example usage demonstrates its correctness by verifying the presence of close elements in a sample list. ```\n",
    "\"\"\"\n",
    "\n",
    "prompt = llm_as_a_judge_prompt_pre + prompt + llm_as_a_judge_prompt_post\n",
    "\n",
    "# print(prompt)\n",
    "# 生成续写文本\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=2000,  # 最大生成长度\n",
    "    min_length=50,   # 最小生成长度\n",
    "    do_sample=True,  # 是否使用采样\n",
    "    early_stopping=True  # 提前停止生成\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf65d8",
   "metadata": {},
   "source": [
    "根据这个评测模型的输出，我们可以对生成的代码产生一些主观上的指标评价，而非仅仅是任务完成率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b8760",
   "metadata": {},
   "source": [
    "\n",
    "## 4.5 常见问题定位与调试\n",
    "当智能体的回答出现错误或不当内容时，我们需要对其进行调试。首先，我们可以检查提示词和上下文示例是否引导了错误。提示词和上下文示例是智能体生成回答的重要依据，如果它们存在问题，可能会导致智能体生成错误的回答。例如，如果提示词不够清晰或准确，可能会使智能体误解任务的要求，从而生成错误的回答。因此，我们需要仔细检查提示词和上下文示例，确保它们能够正确引导智能体生成准确的回答。\n",
    "其次，如果智能体有显式的链式思维输出，我们可以查看其中间思考过程。通过分析中间思考过程，我们可以找出推理链断裂或不合理的地方。例如，智能体在推理过程中可能跳过了某些关键的步骤，或者在某个环节出现了错误的推理。通过逐步验证中间思考过程，我们可以定位问题的来源，并采取相应的措施进行修正。。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd894ecd",
   "metadata": {},
   "source": [
    "拿前面的humaneval输出举例子，虽然它生成的代码是对的，但是太啰嗦了，并且代码后面还跟了一堆解释，非常冗长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d17bd618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "    for i in range(len(numbers)-1):\n",
      "        for j in range(i+1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "\n",
      "    return False\n",
      "\n",
      "# Test cases\n",
      "print(has_close_elements([1.0, 2.0, 3.0], 0.5))  # Expected output: False\n",
      "print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))  # Expected output: True\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "- **Input Validation**: The function `has_close_elements` takes a list of floating-point numbers and a threshold value as inputs.\n",
      "\n",
      "- **Outer Loop**: We iterate through the first element (index 0) of the list with respect to every subsequent element.\n",
      "\n",
      "- **Inner Loop**: For each pair of elements `[numbers[i], numbers[j]]`, we check if the absolute difference between them is less than the given threshold (`threshold`). If so, it means both numbers are close enough, so we return `True`.\n",
      "\n",
      "- **Edge Cases**: In case the entire list or no pairs satisfy the condition, we return `False`.\n",
      "\n",
      "- **Return Statement**: Finally, after checking all possible pairs, we return `False` if no valid pair was found. Otherwise, we return `True`.\n",
      "\n",
      "### Example Usage:\n",
      "\n",
      "```python\n",
      "# Example usage\n",
      "print(has_close_elements([1.0, 2.0, 3.0], 0.5))  # Output: False\n",
      "print(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))  # Output: True\n",
      "```\n",
      "\n",
      "This solution efficiently checks for closeness among pairs of numbers in a single pass through the list, making it suitable for large datasets where direct comparison might be inefficient. The use of nested loops helps to cover all possible pairs, ensuring that any number in the list could potentially form a valid close pair with another number within the specified threshold. \n",
      "\n",
      "The time complexity of this approach is O(n^2), which is optimal given the problem constraints. This makes it very efficient even for larger lists. However, for extremely large datasets, you may want to consider more optimized solutions like using hash sets or implementing an algorithm such as K-means clustering. \n",
      "\n",
      "Keep in mind that while this approach works well, it does not guarantee uniqueness of the closest pair; it only ensures that at least one pair satisfies the condition. To achieve perfect closeness, you would need to use a different method involving sorting and binary search, which can be computationally expensive for large lists. \n",
      "  \n",
      "For practical applications, especially when dealing with large data sets, consider using more sophisticated algorithms or data structures designed specifically for finding the closest pair. These tools often provide better performance characteristics and are widely used in real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device=device  # 指定设备\n",
    ")\n",
    "\n",
    "# 输入提示文本，这里的任务是检查列表中是否有两个数字的差小于给定的阈值，按照要求生成代码\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "from typing import List\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    # Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
    "    # >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    # False\n",
    "    # >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    # True\n",
    "    # Your Code Here\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "# print(prompt)\n",
    "# 生成续写文本\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=2000,  # 最大生成长度\n",
    "    min_length=50,   # 最小生成长度\n",
    "    do_sample=True,  # 是否使用采样\n",
    "    early_stopping=True  # 提前停止生成\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df13ebb",
   "metadata": {},
   "source": [
    "观察模型输出我们可以发现，它后面生成了一堆解释，而这是我们不需要的。也许模型不知道我们需要什么，因为在prompt里我们只提供了代码块以及一部分注释，并没有讲清楚需要什么。因此，我们在prompt里加入了两句话，开头出加入了“generate xxx”告诉模型，我们需要的是生成代码任务；结尾处加入了“You should only give me the response for code.”来告诉大模型，只需要回答代码，而不需要后续解释。效果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30e72b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "```python\n",
      "from typing import List\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    # Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
      "    # >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    # False\n",
      "    # >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    # True\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i+1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "```\n",
      "This solution iterates through all possible pairs of numbers in the given list, calculates their absolute difference using the `abs` function, and checks if this difference is less than the provided threshold. If such a pair exists, it returns True; otherwise, it returns False after checking all pairs. The check function with example usage demonstrates its correctness by verifying the presence of close elements in a sample list. ```\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device=device  # 指定设备\n",
    ")\n",
    "\n",
    "# 输入提示文本，这里的任务是检查列表中是否有两个数字的差小于给定的阈值，按照要求生成代码\n",
    "prompt = \\\n",
    "\"\"\"\n",
    "Generate a Python function that checks if there are any two numbers in a given list that are closer to each other than a specified threshold. The function should take a list of floating-point numbers and a threshold value as input, and return True if such pairs exist, otherwise return False. Include example usage in the docstring.\n",
    "from typing import List\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    # Check if in given list of numbers, are any two numbers closer to each other than given threshold.\n",
    "    # >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    # False\n",
    "    # >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    # True\n",
    "    # Your Code Here\n",
    "    # You should only reponse the code\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "# print(prompt)\n",
    "# 生成续写文本\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=2000,  # 最大生成长度\n",
    "    min_length=50,   # 最小生成长度\n",
    "    do_sample=True,  # 是否使用采样\n",
    "    early_stopping=True  # 提前停止生成\n",
    ")[0]['generated_text']\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9c7d5",
   "metadata": {},
   "source": [
    "通过新的结果，我们可以发现，智能体不再输入冗杂的解释了，而是直接输出代码了，精炼而正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1b4c0",
   "metadata": {},
   "source": [
    "\n",
    "## 4.6 小结\n",
    "本章主要介绍了智能体评估与调试的重要性、方法以及常见问题的处理方式。通过评估和调试，我们可以发现智能体在执行任务时可能出现的问题，并采取相应的措施进行改进。评估方法包括人工测试与用户反馈、自动评估指标以及基准数据集测试。在调试过程中，我们需要检查提示词和上下文示例是否引导了错误，并通过分析中间思考过程来定位问题的来源。\n",
    "为了加深读者对本章内容的理解，我们可以给出一个综合练习项目。要求读者针对之前章节构建的智能体（如第1章问答助手或第3章提示案例）设计测试用例并进行评估。通过这个练习项目，读者可以将本章所学的知识应用到实际中，提高对智能体评估与调试的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c1dfd",
   "metadata": {},
   "source": [
    "### 4.7 课后作业"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a938d",
   "metadata": {},
   "source": [
    "### 选择题\n",
    "\n",
    "1. 以下哪一项最能体现“幻觉（hallucination）”在大语言模型中的表现？\n",
    "\n",
    "    A. 模型生成语法错误的句子\n",
    "\n",
    "    B. 模型回答了不存在的事实信息\n",
    "\n",
    "    C. 模型拒绝回答敏感问题\n",
    "\n",
    "    D. 模型执行速度变慢\n",
    "\n",
    "    答案：B\n",
    "\n",
    "2. 为什么需要对AI智能体进行调试和评估？（可多选）\n",
    "\n",
    "    A. 提高用户体验\n",
    "    \n",
    "    B. 确保模型训练集质量\n",
    "    \n",
    "    C. 发现模型在任务中的潜在问题\n",
    "    \n",
    "    D. 评估优化方法是否有效\n",
    "    \n",
    "    答案：A、C、D\n",
    "\n",
    "3. 以下哪项不是AI智能体与LLM聊天机器人在架构设计上的关键区别？\n",
    "    \n",
    "    A. 智能体具有更低的推理延迟\n",
    "    \n",
    "    B. 智能体运行在复杂环境中\n",
    "    \n",
    "    C. 智能体支持多模态感知\n",
    "    \n",
    "    D. 智能体具有更丰富的反馈机制\n",
    "\n",
    "    答案：A\n",
    "\n",
    "4. 在工具调用场景中，智能体失败的常见原因不包括以下哪项？\n",
    "    \n",
    "    A. 参数缺失\n",
    "    \n",
    "    B. 工具调用顺序错误\n",
    "    \n",
    "    C. 网络带宽不足\n",
    "    \n",
    "    D. 工具文档理解错误 \n",
    "    \n",
    "    答案：C \n",
    "\n",
    "5. 下列关于多源指导的描述，正确的是：\n",
    "    \n",
    "    A. 只来自人类输入的指令\n",
    "    \n",
    "    B. 包括其他智能体协作、层级命令等\n",
    "    \n",
    "    C. 只在图像类任务中适用\n",
    "    \n",
    "    D. 主要用于模型微调\n",
    "    \n",
    "    答案：B\n",
    "\n",
    "6. 关于智能体推理失败的分析，下列说法正确的是？（可多选）\n",
    "    \n",
    "    A. 推理链不一致可能导致最终结论错误\n",
    "    \n",
    "    B. 中间步骤错误可能影响任务完成率\n",
    "    \n",
    "    C. 推理失败不会影响用户体验\n",
    "    \n",
    "    D. 评估中间推理步骤比评估最终答案更重要\n",
    "答案：A、B\n",
    "\n",
    "7. 以下哪一项是自动化评估智能体性能的优点？\n",
    "\n",
    "    \n",
    "    A. 不需要大规模数据标注\n",
    "    \n",
    "    B. 客观性强，适合大规模评估\n",
    "    \n",
    "    C. 更适合应对模型生成内容的伦理问题\n",
    "    \n",
    "    D. 更适合复杂主观任务\n",
    "    \n",
    "    答案：B\n",
    "\n",
    "8. HumanEval基准主要用于评估智能体在什么任务上的能力？\n",
    "    \n",
    "    A. 图像分类\n",
    "    \n",
    "    B. 数学证明\n",
    "    \n",
    "    C. 代码生成\n",
    "    \n",
    "    D. 文本情感分析\n",
    "    \n",
    "    答案：C\n",
    "\n",
    "9. 关于LLM自动评测方式，以下说法错误的是？\n",
    "    \n",
    "    A. 无需人工打标签即可评估模型输出\n",
    "    \n",
    "    B. 具备一定主观分析能力\n",
    "    \n",
    "    C. 速度快，计算资源消耗小\n",
    "    \n",
    "    D. 在复杂任务上也能作为参考标准\n",
    "    \n",
    "    答案：C\n",
    "\n",
    "10. 使用harness库进行智能体评估的主要优势是？\n",
    "    \n",
    "    A. 支持多语言翻译任务优化\n",
    "    \n",
    "    B. 可一键部署多个基准评测任务\n",
    "    \n",
    "    C. 不需要任何环境配置\n",
    "    \n",
    "    D. 能实时收集用户反馈\n",
    "    \n",
    "    答案：B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a9956",
   "metadata": {},
   "source": [
    "### 简答题\n",
    "\n",
    "1. 请解释什么是“幻觉（hallucination）”现象，并举出一个真实案例说明其可能造成的严重后果。\n",
    "\n",
    "2. 智能体在调用外部工具时可能会出现哪些典型问题？请给出三个具体的例子。\n",
    "\n",
    "3. 为什么链式推理中的步骤错误或遗漏会导致严重问题？举一个实际应用场景加以说明。\n",
    "\n",
    "4. 比较AI智能体与传统的LLM聊天机器人在环境感知方面的差异，并讨论为何这一差异对性能评估至关重要。\n",
    "\n",
    "5. 解释并对比人工肉眼评估与自动化评估指标的优缺点，并指出适用场景。\n",
    "\n",
    "6. 什么是基准数据集（Benchmark Dataset），为什么它对评估智能体至关重要？\n",
    "\n",
    "7. 使用你熟悉的编程语言设计一个简单的问题，模拟如何对一个代码生成智能体进行人工评估。\n",
    "\n",
    "8. 请列举并简单说明HumanEval、MMLU和GSM8K这三个基准数据集的用途及其区别。\n",
    "\n",
    "9. 讨论在使用harness库进行基准评测时可能遇到的优势与限制，并给出具体的使用建议。\n",
    "\n",
    "10. 如果你要设计一个自动化评估智能体工具调用能力的基准测试，请描述你会设计哪些指标，并说明如何进行测试。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3e0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
